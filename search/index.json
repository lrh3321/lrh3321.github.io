[{"content":" Ctrl + C: Terminate the command Ctrl + Z: Suspend the command Ctrl + A: Move to the start of the line Ctrl + E: Move to the end of the line Ctrl + F: Move forward one character Ctrl + B: Move backward one character Ctrl + U: Delete from the cursor to the start of the line Ctrl + K: Delete from the cursor to the end of the line Ctrl + W: Delete from the cursor to the start of the word Ctrl + D: Delete the character under the cursor Ctrl + H: Delete the character before the cursor Ctrl + T: Swap the character under the cursor with the previous one Ctrl + L: Clear the screen Editing Shortcuts Action Bash Shortcuts Tab Ctrl + I New Line Ctrl + J Enter Ctrl + M Paste the Last Thing to be Cut Ctrl + Y Undo Ctrl + _ Upper the case Every Character Form Cursor Alt + U Lower the case Every Character Form Cursor Alt + L Capitalize the Character Under Cursor and Move to the End of the Word Alt + C Cancel Changes and Put Back the Line Alt + R Delete Character Under Cursor Ctrl + D Delete Character From Left Ctrl + H Delete Word before Cursor Alt + Delete Delete Word after Cursor Alt + D Clear Screen Ctrl + L Escape Ctrl + [ Cut Word Before Cursor to Clipboard Ctrl + W Cut Line After Cursor to Clipboard Ctrl + K Cut Line Before Cursor to Clipboard Ctrl + U Swap Current Word with Previous Alt + T Swap the Last Character before Cursor Ctrl + T Swap the Last Two Words Before the Cursor Esc + T Emacs and Vi Mode Shortcuts Action Bash Shortcut keys Set Emacs Mode $set -o emacs Set Vi Mode $set -o vi Navigation Shortcuts Action Bash Shortcuts Go to the Beginning of the Line Home Go to the End of the Line End Forward one character Ctrl + F Backward one character Ctrl + B Previous Command Ctrl + P Next Command Ctrl + N Back one word Alt + B Forward one word Alt + F Control Character Shortcuts Action Bash Shortcuts ^@ Ctrl + 2 ^Escape Ctrl + 3 ^\\ Ctrl + 4 ^ Ctrl + 5 ^^ Ctrl + 6 ^_Undo Ctrl + 7 ^? Backward Delete Character Ctrl + 8 Display Sequence for entering Key Ctrl + V History Shortcuts Action Bash Shortcuts Recall Last Command Ctrl + R Previous Command in History Ctrl + P Next Command in History Ctrl + N Go back to Next Most Recent Command Ctrl + S Escape from History Searching Mode Ctrl + O Repeat Last Command Ctrl + G Run Last Command Starting with ABC !! Print the last Command starting with ABC !abc Last Argument of Previous Command !abc:p Last Argument of Previous Command !$ All Arguments of Previous Command Alt + . Run Previous Command, replacing abc with def ^abc^def Process Controls Shortcuts Action Bash Shortcuts Recall Last Command Ctrl + R Interrupt or Kill Whatever you are running Ctrl + C Clear Screen Ctrl + L Stop Output to Screen Ctrl + S Allow Output to Screen Ctrl + Q Send an EOF Marker Ctrl + D Send the signal SIGTSTP to Current Task Ctrl + Z The bash shortcuts will boost maximizing your productivity to around 90%. However, Bash is not the only shell out there. The one that can be the best alternative to bash is ZSH. It is a default shell in macOS. No doubt the experience of using this shell can be quite different. Dig into the ZSH shortcuts list.\n/etc/inputrc readline(3) — Linux manual page\nGNU Readline Library\n8.4 Bindable Readline Commands\nCommands For Moving beginning-of-line (C-a)\nMove to the start of the current line.\nend-of-line (C-e)\nMove to the end of the line.\nforward-char (C-f)\nMove forward a character.\nbackward-char (C-b)\nMove back a character.\nforward-word (M-f)\nMove forward to the end of the next word. Words are composed of letters and digits.\nbackward-word (M-b)\nMove back to the start of the current or previous word. Words are composed of letters and digits.\nshell-forward-word (M-C-f)\nMove forward to the end of the next word. Words are delimited by non-quoted shell metacharacters.\nshell-backward-word (M-C-b)\nMove back to the start of the current or previous word. Words are delimited by non-quoted shell metacharacters.\nprevious-screen-line ()\nAttempt to move point to the same physical screen column on the previous physical screen line. This will not have the desired effect if the current Readline line does not take up more than one physical line or if point is not greater than the length of the prompt plus the screen width.\nnext-screen-line ()\nAttempt to move point to the same physical screen column on the next physical screen line. This will not have the desired effect if the current Readline line does not take up more than one physical line or if the length of the current Readline line is not greater than the length of the prompt plus the screen width.\nclear-display (M-C-l)\nClear the screen and, if possible, the terminal’s scrollback buffer, then redraw the current line, leaving the current line at the top of the screen.\nclear-screen (C-l)\nClear the screen, then redraw the current line, leaving the current line at the top of the screen.\nredraw-current-line ()\nRefresh the current line. By default, this is unbound.\nCommands For Manipulating The History accept-line (Newline or Return) Accept the line regardless of where the cursor is. If this line is non-empty, add it to the history list according to the setting of the HISTCONTROL and HISTIGNORE variables. If this line is a modified history line, then restore the history line to its original state.\nprevious-history (C-p)\nMove ‘back’ through the history list, fetching the previous command.\nnext-history (C-n)\nMove ‘forward’ through the history list, fetching the next command.\nbeginning-of-history (M-\u0026lt;)\nMove to the first line in the history.\nend-of-history (M-\u0026gt;)\nMove to the end of the input history, i.e., the line currently being entered.\nreverse-search-history (C-r)\nSearch backward starting at the current line and moving ‘up’ through the history as necessary. This is an incremental search. This command sets the region to the matched text and activates the mark.\nforward-search-history (C-s)\nSearch forward starting at the current line and moving ‘down’ through the history as necessary. This is an incremental search. This command sets the region to the matched text and activates the mark.\nnon-incremental-reverse-search-history (M-p)\nSearch backward starting at the current line and moving ‘up’ through the history as necessary using a non-incremental search for a string supplied by the user. The search string may match anywhere in a history line.\nnon-incremental-forward-search-history (M-n)\nSearch forward starting at the current line and moving ‘down’ through the history as necessary using a non-incremental search for a string supplied by the user. The search string may match anywhere in a history line.\nhistory-search-forward ()\nSearch forward through the history for the string of characters between the start of the current line and the point. The search string must match at the beginning of a history line. This is a non-incremental search. By default, this command is unbound.\nhistory-search-backward ()\nSearch backward through the history for the string of characters between the start of the current line and the point. The search string must match at the beginning of a history line. This is a non-incremental search. By default, this command is unbound.\nhistory-substring-search-forward ()\nSearch forward through the history for the string of characters between the start of the current line and the point. The search string may match anywhere in a history line. This is a non-incremental search. By default, this command is unbound.\nhistory-substring-search-backward ()\nSearch backward through the history for the string of characters between the start of the current line and the point. The search string may match anywhere in a history line. This is a non-incremental search. By default, this command is unbound.\nyank-nth-arg (M-C-y)\nInsert the first argument to the previous command (usually the second word on the previous line) at point. With an argument n, insert the nth word from the previous command (the words in the previous command begin with word 0). A negative argument inserts the nth word from the end of the previous command. Once the argument n is computed, the argument is extracted as if the ‘!n’ history expansion had been specified.\nyank-last-arg (M-. or M-_)\nInsert last argument to the previous command (the last word of the previous history entry). With a numeric argument, behave exactly like yank-nth-arg. Successive calls to yank-last-arg move back through the history list, inserting the last word (or the word specified by the argument to the first call) of each line in turn. Any numeric argument supplied to these successive calls determines the direction to move through the history. A negative argument switches the direction through the history (back or forward). The history expansion facilities are used to extract the last argument, as if the ‘!$’ history expansion had been specified.\noperate-and-get-next (C-o)\nAccept the current line for return to the calling application as if a newline had been entered, and fetch the next line relative to the current line from the history for editing. A numeric argument, if supplied, specifies the history entry to use instead of the current line.\nfetch-history ()\nWith a numeric argument, fetch that entry from the history list and make it the current line. Without an argument, move back to the first entry in the history list.\nCommands For Changing Text end-of-file (usually C-d)\nThe character indicating end-of-file as set, for example, by stty. If this character is read when there are no characters on the line, and point is at the beginning of the line, Readline interprets it as the end of input and returns EOF.\ndelete-char (C-d)\nDelete the character at point. If this function is bound to the same character as the tty EOF character, as C-d commonly is, see above for the effects.\nbackward-delete-char (Rubout)\nDelete the character behind the cursor. A numeric argument means to kill the characters instead of deleting them.\nforward-backward-delete-char ()\nDelete the character under the cursor, unless the cursor is at the end of the line, in which case the character behind the cursor is deleted. By default, this is not bound to a key.\nquoted-insert (C-q or C-v)\nAdd the next character typed to the line verbatim. This is how to insert key sequences like C-q, for example.\nself-insert (a, b, A, 1, !, …)\nInsert yourself.\nbracketed-paste-begin ()\nThis function is intended to be bound to the \u0026ldquo;bracketed paste\u0026rdquo; escape sequence sent by some terminals, and such a binding is assigned by default. It allows Readline to insert the pasted text as a single unit without treating each character as if it had been read from the keyboard. The characters are inserted as if each one was bound to self-insert instead of executing any editing commands.\nBracketed paste sets the region (the characters between point and the mark) to the inserted text. It uses the concept of an active mark: when the mark is active, Readline redisplay uses the terminal’s standout mode to denote the region.\ntranspose-chars (C-t)\nDrag the character before the cursor forward over the character at the cursor, moving the cursor forward as well. If the insertion point is at the end of the line, then this transposes the last two characters of the line. Negative arguments have no effect.\ntranspose-words (M-t)\nDrag the word before point past the word after point, moving point past that word as well. If the insertion point is at the end of the line, this transposes the last two words on the line.\nupcase-word (M-u)\nUppercase the current (or following) word. With a negative argument, uppercase the previous word, but do not move the cursor.\ndowncase-word (M-l)\nLowercase the current (or following) word. With a negative argument, lowercase the previous word, but do not move the cursor.\ncapitalize-word (M-c)\nCapitalize the current (or following) word. With a negative argument, capitalize the previous word, but do not move the cursor.\noverwrite-mode ()\nToggle overwrite mode. With an explicit positive numeric argument, switches to overwrite mode. With an explicit non-positive numeric argument, switches to insert mode. This command affects only emacs mode; vi mode does overwrite differently. Each call to readline() starts in insert mode.\nIn overwrite mode, characters bound to self-insert replace the text at point rather than pushing the text to the right. Characters bound to backward-delete-char replace the character before point with a space.\nBy default, this command is unbound.\nKilling And Yanking kill-line (C-k)\nKill the text from point to the end of the line. With a negative numeric argument, kill backward from the cursor to the beginning of the current line.\nbackward-kill-line (C-x Rubout)\nKill backward from the cursor to the beginning of the current line. With a negative numeric argument, kill forward from the cursor to the end of the current line.\nunix-line-discard (C-u)\nKill backward from the cursor to the beginning of the current line.\nkill-whole-line ()\nKill all characters on the current line, no matter where point is. By default, this is unbound.\nkill-word (M-d)\nKill from point to the end of the current word, or if between words, to the end of the next word. Word boundaries are the same as forward-word.\nbackward-kill-word (M-DEL)\nKill the word behind point. Word boundaries are the same as backward-word.\nshell-kill-word (M-C-d)\nKill from point to the end of the current word, or if between words, to the end of the next word. Word boundaries are the same as shell-forward-word.\nshell-backward-kill-word ()\nKill the word behind point. Word boundaries are the same as shell-backward-word.\nshell-transpose-words (M-C-t)\nDrag the word before point past the word after point, moving point past that word as well. If the insertion point is at the end of the line, this transposes the last two words on the line. Word boundaries are the same as shell-forward-word and shell-backward-word.\nunix-word-rubout (C-w)\nKill the word behind point, using white space as a word boundary. The killed text is saved on the kill-ring.\nunix-filename-rubout ()\nKill the word behind point, using white space and the slash character as the word boundaries. The killed text is saved on the kill-ring.\ndelete-horizontal-space ()\nDelete all spaces and tabs around point. By default, this is unbound.\nkill-region ()\nKill the text in the current region. By default, this command is unbound.\ncopy-region-as-kill ()\nCopy the text in the region to the kill buffer, so it can be yanked right away. By default, this command is unbound.\ncopy-backward-word ()\nCopy the word before point to the kill buffer. The word boundaries are the same as backward-word. By default, this command is unbound.\ncopy-forward-word ()\nCopy the word following point to the kill buffer. The word boundaries are the same as forward-word. By default, this command is unbound.\nyank (C-y)\nYank the top of the kill ring into the buffer at point.\nyank-pop (M-y)\nRotate the kill-ring, and yank the new top. You can only do this if the prior command is yank or yank-pop.\nSpecifying Numeric Arguments digit-argument (M-0, M-1, … M--)\nAdd this digit to the argument already accumulating, or start a new argument. M\u0026ndash; starts a negative argument.\nuniversal-argument ()\nThis is another way to specify an argument. If this command is followed by one or more digits, optionally with a leading minus sign, those digits define the argument. If the command is followed by digits, executing universal-argument again ends the numeric argument, but is otherwise ignored. As a special case, if this command is immediately followed by a character that is neither a digit nor minus sign, the argument count for the next command is multiplied by four. The argument count is initially one, so executing this function the first time makes the argument count four, a second time makes the argument count sixteen, and so on. By default, this is not bound to a key.\nLetting Readline Type For You complete (TAB) Attempt to perform completion on the text before point. The actual completion performed is application-specific. Bash attempts completion treating the text as a variable (if the text begins with ‘$’), username (if the text begins with ‘~’), hostname (if the text begins with ‘@’), or command (including aliases and functions) in turn. If none of these produces a match, filename completion is attempted.\npossible-completions (M-?)\nList the possible completions of the text before point. When displaying completions, Readline sets the number of columns used for display to the value of completion-display-width, the value of the environment variable COLUMNS, or the screen width, in that order.\ninsert-completions (M-*)\nInsert all completions of the text before point that would have been generated by possible-completions.\nmenu-complete ()\nSimilar to complete, but replaces the word to be completed with a single match from the list of possible completions. Repeated execution of menu-complete steps through the list of possible completions, inserting each match in turn. At the end of the list of completions, the bell is rung (subject to the setting of bell-style) and the original text is restored. An argument of n moves n positions forward in the list of matches; a negative argument may be used to move backward through the list. This command is intended to be bound to TAB, but is unbound by default.\nmenu-complete-backward ()\nIdentical to menu-complete, but moves backward through the list of possible completions, as if menu-complete had been given a negative argument.\ndelete-char-or-list ()\nDeletes the character under the cursor if not at the beginning or end of the line (like delete-char). If at the end of the line, behaves identically to possible-completions. This command is unbound by default.\ncomplete-filename (M-/)\nAttempt filename completion on the text before point.\npossible-filename-completions (C-x /)\nList the possible completions of the text before point, treating it as a filename.\ncomplete-username (M-~)\nAttempt completion on the text before point, treating it as a username.\npossible-username-completions (C-x ~)\nList the possible completions of the text before point, treating it as a username.\ncomplete-variable (M-$)\nAttempt completion on the text before point, treating it as a shell variable.\npossible-variable-completions (C-x $)\nList the possible completions of the text before point, treating it as a shell variable.\ncomplete-hostname (M-@)\nAttempt completion on the text before point, treating it as a hostname.\npossible-hostname-completions (C-x @)\nList the possible completions of the text before point, treating it as a hostname.\ncomplete-command (M-!)\nAttempt completion on the text before point, treating it as a command name. Command completion attempts to match the text against aliases, reserved words, shell functions, shell builtins, and finally executable filenames, in that order.\npossible-command-completions (C-x !)\nList the possible completions of the text before point, treating it as a command name.\ndynamic-complete-history (M-TAB)\nAttempt completion on the text before point, comparing the text against lines from the history list for possible completion matches.\ndabbrev-expand ()\nAttempt menu completion on the text before point, comparing the text against lines from the history list for possible completion matches.\ncomplete-into-braces (M-{)\nPerform filename completion and insert the list of possible completions enclosed within braces so the list is available to the shell (see Brace Expansion).\nKeyboard Macros start-kbd-macro (C-x ()\nBegin saving the characters typed into the current keyboard macro.\nend-kbd-macro (C-x ))\nStop saving the characters typed into the current keyboard macro and save the definition.\ncall-last-kbd-macro (C-x e)\nRe-execute the last keyboard macro defined, by making the characters in the macro appear as if typed at the keyboard.\nprint-last-kbd-macro ()\nPrint the last keyboard macro defined in a format suitable for the inputrc file.\nSome Miscellaneous Commands re-read-init-file (C-x C-r)\nRead in the contents of the inputrc file, and incorporate any bindings or variable assignments found there.\nabort (C-g)\nAbort the current editing command and ring the terminal’s bell (subject to the setting of bell-style).\ndo-lowercase-version (M-A, M-B, M-x, …)\nIf the metafied character x is upper case, run the command that is bound to the corresponding metafied lower case character. The behavior is undefined if x is already lower case.\nprefix-meta (ESC)\nMetafy the next character typed. This is for keyboards without a meta key. Typing ‘ESC f’ is equivalent to typing M-f.\nundo (C-_ or C-x C-u)\nIncremental undo, separately remembered for each line.\nrevert-line (M-r)\nUndo all changes made to this line. This is like executing the undo command enough times to get back to the beginning.\ntilde-expand (M-\u0026amp;)\nPerform tilde expansion on the current word.\nset-mark (C-@)\nSet the mark to the point. If a numeric argument is supplied, the mark is set to that position.\nexchange-point-and-mark (C-x C-x)\nSwap the point with the mark. The current cursor position is set to the saved position, and the old cursor position is saved as the mark.\ncharacter-search (C-])\nA character is read and point is moved to the next occurrence of that character. A negative argument searches for previous occurrences.\ncharacter-search-backward (M-C-])\nA character is read and point is moved to the previous occurrence of that character. A negative argument searches for subsequent occurrences.\nskip-csi-sequence ()\nRead enough characters to consume a multi-key sequence such as those defined for keys like Home and End. Such sequences begin with a Control Sequence Indicator (CSI), usually ESC-[. If this sequence is bound to \u0026ldquo;\\e[\u0026rdquo;, keys producing such sequences will have no effect unless explicitly bound to a Readline command, instead of inserting stray characters into the editing buffer. This is unbound by default, but usually bound to ESC-[.\ninsert-comment (M-#)\nWithout a numeric argument, the value of the comment-begin variable is inserted at the beginning of the current line. If a numeric argument is supplied, this command acts as a toggle: if the characters at the beginning of the line do not match the value of comment-begin, the value is inserted, otherwise the characters in comment-begin are deleted from the beginning of the line. In either case, the line is accepted as if a newline had been typed. The default value of comment-begin causes this command to make the current line a shell comment. If a numeric argument causes the comment character to be removed, the line will be executed by the shell.\ndump-functions ()\nPrint all of the functions and their key bindings to the Readline output stream. If a numeric argument is supplied, the output is formatted in such a way that it can be made part of an inputrc file. This command is unbound by default.\ndump-variables ()\nPrint all of the settable variables and their values to the Readline output stream. If a numeric argument is supplied, the output is formatted in such a way that it can be made part of an inputrc file. This command is unbound by default.\ndump-macros ()\nPrint all of the Readline key sequences bound to macros and the strings they output. If a numeric argument is supplied, the output is formatted in such a way that it can be made part of an inputrc file. This command is unbound by default.\nspell-correct-word (C-x s)\nPerform spelling correction on the current word, treating it as a directory or filename, in the same way as the cdspell shell option. Word boundaries are the same as those used by shell-forward-word.\nglob-complete-word (M-g)\nThe word before point is treated as a pattern for pathname expansion, with an asterisk implicitly appended. This pattern is used to generate a list of matching file names for possible completions.\nglob-expand-word (C-x *)\nThe word before point is treated as a pattern for pathname expansion, and the list of matching file names is inserted, replacing the word. If a numeric argument is supplied, a ‘*’ is appended before pathname expansion.\nglob-list-expansions (C-x g)\nThe list of expansions that would have been generated by glob-expand-word is displayed, and the line is redrawn. If a numeric argument is supplied, a ‘*’ is appended before pathname expansion.\ndisplay-shell-version (C-x C-v)\nDisplay version information about the current instance of Bash.\nshell-expand-line (M-C-e)\nExpand the line as the shell does. This performs alias and history expansion as well as all of the shell word expansions (see Shell Expansions).\nhistory-expand-line (M-^)\nPerform history expansion on the current line.\nmagic-space ()\nPerform history expansion on the current line and insert a space (see History Expansion).\nalias-expand-line ()\nPerform alias expansion on the current line (see Aliases).\nhistory-and-alias-expand-line ()\nPerform history and alias expansion on the current line.\ninsert-last-argument (M-. or M-_)\nA synonym for yank-last-arg.\nedit-and-execute-command (C-x C-e)\nInvoke an editor on the current command line, and execute the result as shell commands. Bash attempts to invoke $VISUAL, $EDITOR, and emacs as the editor, in that order.\nhttps://tutorialtactic.com/keyboard-shortcuts/bash-shortcuts/\nhttps://github.com/fliptheweb/bash-shortcuts-cheat-sheet\nhttps://www.gnu.org/software/bash/manual/bash.html#Readline-Init-File-Syntax-1\n","date":"2023-08-30T22:07:48+08:00","permalink":"https://www.lrh3321.win/p/bash-shortcuts/","title":"Bash Shortcuts"},{"content":"使用 Fio 进行磁盘性能测试 Flexible I/O Tester\n现在 Fio 已经安装到了你的系统中。现在是时候看一些如何使用 Fio 的例子了。我们将进行随机写、读和读写测试。\n执行随机写测试 执行下面的命令来开始。这个命令将要同一时间执行两个进程，写入共计 4GB（ 4 个任务 x 512MB = 2GB） 文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 sudo fio --name=randwrite --ioengine=libaio --iodepth=1 --rw=randwrite --bs=4k --direct=0 --size=512M --numjobs=2 --runtime=240 --group_reporting ... fio-2.2.10 Starting 2 processes randwrite: (groupid=0, jobs=2): err= 0: pid=7271: Sat Aug 5 13:28:44 2017 write: io=1024.0MB, bw=2485.5MB/s, iops=636271, runt= 412msec slat (usec): min=1, max=268, avg= 1.79, stdev= 1.01 clat (usec): min=0, max=13, avg= 0.20, stdev= 0.40 lat (usec): min=1, max=268, avg= 2.03, stdev= 1.01 clat percentiles (usec): | 1.00th=[ 0], 5.00th=[ 0], 10.00th=[ 0], 20.00th=[ 0], | 30.00th=[ 0], 40.00th=[ 0], 50.00th=[ 0], 60.00th=[ 0], | 70.00th=[ 0], 80.00th=[ 1], 90.00th=[ 1], 95.00th=[ 1], | 99.00th=[ 1], 99.50th=[ 1], 99.90th=[ 1], 99.95th=[ 1], | 99.99th=[ 1] lat (usec) : 2=99.99%, 4=0.01%, 10=0.01%, 20=0.01% cpu : usr=15.14%, sys=84.00%, ctx=8, majf=0, minf=26 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% issued : total=r=0/w=262144/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: io=1024.0MB, aggrb=2485.5MB/s, minb=2485.5MB/s, maxb=2485.5MB/s, mint=412msec, maxt=412msec Disk stats (read/write): sda: ios=0/0, merge=0/0, ticks=0/0, in_queue=0, util=0.00% 执行随机读测试 我们将要执行一个随机读测试，我们将会尝试读取一个随机的 2GB 文件。\n1 sudo fio --name=randread --ioengine=libaio --iodepth=16 --rw=randread --bs=4k --direct=0 --size=512M --numjobs=4 --runtime=240 --group_reporting 你应该会看到下面这样的输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ... fio-2.2.10 Starting 4 processes randread: Laying out IO file(s) (1 file(s) / 512MB) randread: Laying out IO file(s) (1 file(s) / 512MB) randread: Laying out IO file(s) (1 file(s) / 512MB) randread: Laying out IO file(s) (1 file(s) / 512MB) Jobs: 4 (f=4): [r(4)] [100.0% done] [71800KB/0KB/0KB /s] [17.1K/0/0 iops] [eta 00m:00s] randread: (groupid=0, jobs=4): err= 0: pid=7586: Sat Aug 5 13:30:52 2017 read : io=2048.0MB, bw=80719KB/s, iops=20179, runt= 25981msec slat (usec): min=72, max=10008, avg=195.79, stdev=94.72 clat (usec): min=2, max=28811, avg=2971.96, stdev=760.33 lat (usec): min=185, max=29080, avg=3167.96, stdev=798.91 clat percentiles (usec): | 1.00th=[ 2192], 5.00th=[ 2448], 10.00th=[ 2576], 20.00th=[ 2736], | 30.00th=[ 2800], 40.00th=[ 2832], 50.00th=[ 2928], 60.00th=[ 3024], | 70.00th=[ 3120], 80.00th=[ 3184], 90.00th=[ 3248], 95.00th=[ 3312], | 99.00th=[ 3536], 99.50th=[ 6304], 99.90th=[15168], 99.95th=[18816], | 99.99th=[22912] bw (KB /s): min=17360, max=25144, per=25.05%, avg=20216.90, stdev=1605.65 lat (usec) : 4=0.01%, 10=0.01%, 250=0.01%, 500=0.01%, 750=0.01% lat (usec) : 1000=0.01% lat (msec) : 2=0.01%, 4=99.27%, 10=0.44%, 20=0.24%, 50=0.04% cpu : usr=1.35%, sys=5.18%, ctx=524309, majf=0, minf=98 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=100.0%, 32=0.0%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% issued : total=r=524288/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=16 Run status group 0 (all jobs): READ: io=2048.0MB, aggrb=80718KB/s, minb=80718KB/s, maxb=80718KB/s, mint=25981msec, maxt=25981msec Disk stats (read/write): sda: ios=521587/871, merge=0/1142, ticks=96664/612, in_queue=97284, util=99.85% 最后，我们想要展示一个简单的随机读-写测试来看一看 Fio 返回的输出类型。\n读写性能测试 下述命令将会测试 USB Pen 驱动器 (/dev/sdc1) 的随机读写性能：\n1 sudo fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=random_read_write.fio --bs=4k --iodepth=64 --size=4G --readwrite=randrw --rwmixread=75 下面的内容是我们从上面的命令得到的输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 fio-2.2.10 Starting 1 process Jobs: 1 (f=1): [m(1)] [100.0% done] [217.8MB/74452KB/0KB /s] [55.8K/18.7K/0 iops] [eta 00m:00s] test: (groupid=0, jobs=1): err= 0: pid=8475: Sat Aug 5 13:36:04 2017 read : io=3071.7MB, bw=219374KB/s, iops=54843, runt= 14338msec write: io=1024.4MB, bw=73156KB/s, iops=18289, runt= 14338msec cpu : usr=6.78%, sys=20.81%, ctx=1007218, majf=0, minf=9 IO depths : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, \u0026gt;=64=100.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, \u0026gt;=64=0.0% issued : total=r=786347/w=262229/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0 latency : target=0, window=0, percentile=100.00%, depth=64 Run status group 0 (all jobs): READ: io=3071.7MB, aggrb=219374KB/s, minb=219374KB/s, maxb=219374KB/s, mint=14338msec, maxt=14338msec WRITE: io=1024.4MB, aggrb=73156KB/s, minb=73156KB/s, maxb=73156KB/s, mint=14338msec, maxt=14338msec Disk stats (read/write): sda: ios=774141/258944, merge=1463/899, ticks=748800/150316, in_queue=900720, util=99.35% 使用配置文件 1 fio fio.conf Linux 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 [global] ioengine=libaio direct=1 thread=1 time_based numjobs=1 group_reporting iodepth=128 filename=/dev/vdb runtime=300 size=1g [4k-randwrite] bs=4k rw=randwrite size=256m numjobs=1 iodepth=1 stonewall [8k-randwrite] bs=8k size=256m numjobs=1 iodepth=1 rw=randwrite stonewall [64k-16p-randwrite] bs=64k size=256m numjobs=16 iodepth=16 rw=randwrite stonewall [4k-write] bs=4k rw=write size=256m numjobs=1 iodepth=1 stonewall [8k-write] bs=8k size=256m numjobs=1 iodepth=1 rw=write stonewall [64k-16p-write] bs=64k size=256m numjobs=16 iodepth=16 rw=write stonewall [4k-randread] bs=4k rw=randread size=256m numjobs=1 iodepth=1 stonewall [8k-randread] bs=8k size=256m numjobs=1 iodepth=1 rw=randread stonewall [64k-16p-randread] bs=64k size=256m numjobs=16 iodepth=16 rw=randread stonewall [4k-read] bs=4k rw=read size=256m numjobs=1 iodepth=1 stonewall [8k-read] bs=8k size=256m numjobs=1 iodepth=1 rw=read stonewall [64k-16p-read] bs=64k size=256m numjobs=16 iodepth=16 rw=read stonewall Windows 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 [global] ioengine=windowsaio direct=1 thread=1 time_based numjobs=1 group_reporting iodepth=128 filename=fiojob runtime=300 size=1g [4k-randwrite] bs=4k rw=randwrite size=256m numjobs=1 iodepth=1 stonewall [8k-randwrite] bs=8k size=256m numjobs=1 iodepth=1 rw=randwrite stonewall [64k-16p-randwrite] bs=64k size=256m numjobs=16 iodepth=16 rw=randwrite stonewall [4k-write] bs=4k rw=write size=256m numjobs=1 iodepth=1 stonewall [8k-write] bs=8k size=256m numjobs=1 iodepth=1 rw=write stonewall [64k-16p-write] bs=64k size=256m numjobs=16 iodepth=16 rw=write stonewall [4k-randread] bs=4k rw=randread size=256m numjobs=1 iodepth=1 stonewall [8k-randread] bs=8k size=256m numjobs=1 iodepth=1 rw=randread stonewall [64k-16p-randread] bs=64k size=256m numjobs=16 iodepth=16 rw=randread stonewall [4k-read] bs=4k rw=read size=256m numjobs=1 iodepth=1 stonewall [8k-read] bs=8k size=256m numjobs=1 iodepth=1 rw=read stonewall [64k-16p-read] bs=64k size=256m numjobs=16 iodepth=16 rw=read stonewall 如何在 Linux 中使用 Fio 来测评硬盘性能 How fast are your disks? Find out the open source way, with fio fio(1) - Linux man page ","date":"2023-07-15T11:10:37+08:00","permalink":"https://www.lrh3321.win/p/fio/","title":"使用 Fio 进行磁盘性能测试"},{"content":"使用 IPMI 管理 Linux 服务器 安装 ipmitool Debian 系 1 apt install -y ipmitool RHEL 系 1 2 3 yum install -y ipmitool # 或 dnf install -y ipmitool 1 2 3 4 5 6 # modprobe ipmi_msghandler # modprobe ipmi_devintf # modprobe ipmi_si ipmitool user list 1 ipmitool user list 2 使用命令设置IPMI地址 1 2 3 4 5 6 7 8 # 查看当前设置 ipmitool lan print 1 ipmitool lan print 8 ipmitool lan set 1 ipsrc static #设置IPMI的IP地址配置方式未静态IP ipmitool lan set 1 ipaddr 192.168.21.200 #设置IPMI的IP地址 ipmitool lan set 1 netmask 255.255.255.0 #设置IPMI的掩码 ipmitool lan set 1 defgw ipaddr 192.168.21.1 #设置IPMI的网关 重置服务器硬件管理口 IMM IPMI BMC 密码 1 ipmitool user set password 2 Lrh@3321 常见服务器 IPMI 初始密码 DELL 服务器: root\\ calvin root \\ root 浪潮服务器: root \\ superuser 泰安主板: root \\ superuser 超微主板: ADMIN \\ ADMIN Lenvon: USERID \\ PASSW0RD 远程管理其他服务器 1 2 3 4 5 6 7 8 9 bmc_ip=\u0026#34;\u0026#34; bmc_user=\u0026#34;root\u0026#34; bmc_passwd=\u0026#34;root\u0026#34; args=\u0026#34;\u0026#34; # 要执行的参数 ipmitool -H ${bmc_ip} -I lanplus -U ${bmc_user} -P ${bmc_passwd} ${args} alias remote_ipmitool=\u0026#39;ipmitool -H ${bmc_ip} -I lanplus -U ${bmc_user} -P ${bmc_passwd}\u0026#39; 服务器 I IPMI 密码重置及初始密码 使用命令设置IPMI地址 重置服务器硬件管理口 IMM IPMI BMC 密码 ","date":"2023-07-15T11:08:58+08:00","permalink":"https://www.lrh3321.win/p/ipmi/","title":"使用 IPMI 管理 Linux 服务器"},{"content":"打开 Powershell Profile 配置文件\n1 code $PROFILE.CurrentUserAllHosts which 1 2 3 4 5 Function Show-Full-Path { $(Get-Command $args[0]).Source; } Set-Alias -Name which -Value Show-Full-Path grep 1 Set-Alias -Name grep -Value Select-String rg rg\nripgrep recursively searches directories for a regex pattern while respecting your gitignore\nLatest releases\n1 cargo install ripgrep 1 Set-Alias -Name grep -Value rg find findutils findutils\nRust implementation of findutils\nInstall 1 cargo install findutils fd fd\nA simple, fast and user-friendly alternative to \u0026lsquo;find\u0026rsquo;\nInstall 1 cargo install fd-find 1 winget install sharkdp.fd 1 Set-Alias -Name find -Value fd tree erd erd\nA modern, multi-threaded file-tree visualization and disk usage analysis tool that respects hidden file and gitignore rules.\n1 cargo install erdtree 1 Set-Alias -Name tree -Value erd ls lsd lsd\nThe next gen ls command\n1 cargo install lsd 1 Set-Alias -Name ls -Value lsd dig dog dog\nA command-line DNS client.\n1 cargo install --git https://github.com/ogham/dog.git dog 1 Set-Alias -Name dig -Value dog coreutils coreutils\nuutils Coreutils Documentation\nCross-platform Rust rewrite of the GNU coreutils\n1 cargo install coreutils 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # [, arch, b2sum, b3sum, base32, base64, basename, basenc, cat, cksum, comm, cp, csplit, cut, # date, dd, df, dir, dircolors, dirname, du, echo, env, expand, expr, factor, false, fmt, # fold, hashsum, head, hostname, join, link, ln, ls, md5sum, mkdir, mktemp, more, mv, nl, # nproc, numfmt, od, paste, pr, printenv, printf, ptx, pwd, readlink, realpath, relpath, rm, # rmdir, seq, sha1sum, sha224sum, sha256sum, sha3-224sum, sha3-256sum, sha3-384sum, sha3- # 512sum, sha384sum, sha3sum, sha512sum, shake128sum, shake256sum, shred, shuf, sleep, sort, # split, sum, sync, tac, tail, tee, test, touch, tr, true, truncate, tsort, uname, unexpand, # uniq, unlink, vdir, wc, whoami, yes Set-Location $(Split-Path -Path $(Get-Command coreutils).Source) $comands = \u0026#34;[\u0026#34;, \u0026#34;arch\u0026#34;, \u0026#34;b2sum\u0026#34;, \u0026#34;b3sum\u0026#34;, \u0026#34;base32\u0026#34;, \u0026#34;base64\u0026#34;, \u0026#34;basename\u0026#34;, \u0026#34;basenc\u0026#34;, \u0026#34;cksum\u0026#34;, \u0026#34;comm\u0026#34;, \u0026#34;csplit\u0026#34;, \u0026#34;cut\u0026#34; $comands += \u0026#34;date\u0026#34;, \u0026#34;dd\u0026#34;, \u0026#34;df\u0026#34;, \u0026#34;dir\u0026#34;, \u0026#34;dircolors\u0026#34;, \u0026#34;dirname\u0026#34;, \u0026#34;du\u0026#34;, \u0026#34;echo\u0026#34;, \u0026#34;env\u0026#34;, \u0026#34;expand\u0026#34;, \u0026#34;expr\u0026#34;, \u0026#34;factor\u0026#34;, \u0026#34;false\u0026#34;, \u0026#34;fmt\u0026#34; $comands += \u0026#34;fold\u0026#34;, \u0026#34;hashsum\u0026#34;, \u0026#34;head\u0026#34;, \u0026#34;hostname\u0026#34;, \u0026#34;join\u0026#34;, \u0026#34;link\u0026#34;, \u0026#34;ln\u0026#34;, \u0026#34;ls\u0026#34;, \u0026#34;md5sum\u0026#34;, \u0026#34;mkdir\u0026#34;, \u0026#34;mktemp\u0026#34;, \u0026#34;more\u0026#34;, \u0026#34;mv\u0026#34;, \u0026#34;nl\u0026#34; $comands += \u0026#34;nproc\u0026#34;, \u0026#34;numfmt\u0026#34;, \u0026#34;od\u0026#34;, \u0026#34;paste\u0026#34;, \u0026#34;pr\u0026#34;, \u0026#34;printenv\u0026#34;, \u0026#34;printf\u0026#34;, \u0026#34;ptx\u0026#34;, \u0026#34;pwd\u0026#34;, \u0026#34;readlink\u0026#34;, \u0026#34;realpath\u0026#34;, \u0026#34;relpath\u0026#34;, \u0026#34;rm\u0026#34; $comands += \u0026#34;rmdir\u0026#34;, \u0026#34;seq\u0026#34;, \u0026#34;sha1sum\u0026#34;, \u0026#34;sha224sum\u0026#34;, \u0026#34;sha256sum\u0026#34;, \u0026#34;sha3-224sum\u0026#34;, \u0026#34;sha3-256sum\u0026#34;, \u0026#34;sha3-384sum\u0026#34;, \u0026#34;sha3-512sum\u0026#34; $comands += \u0026#34;sha384sum\u0026#34;, \u0026#34;sha3sum\u0026#34;, \u0026#34;sha512sum\u0026#34;, \u0026#34;shake128sum\u0026#34;, \u0026#34;shake256sum\u0026#34;, \u0026#34;shred\u0026#34;, \u0026#34;shuf\u0026#34;, \u0026#34;sleep\u0026#34;, \u0026#34;sort\u0026#34; $comands += \u0026#34;split\u0026#34;, \u0026#34;sum\u0026#34;, \u0026#34;sync\u0026#34;, \u0026#34;tac\u0026#34;, \u0026#34;tail\u0026#34;, \u0026#34;tee\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;touch\u0026#34;, \u0026#34;tr\u0026#34;, \u0026#34;true\u0026#34;, \u0026#34;truncate\u0026#34;, \u0026#34;tsort\u0026#34;, \u0026#34;uname\u0026#34;, \u0026#34;unexpand\u0026#34; $comands += \u0026#34;uniq\u0026#34;, \u0026#34;unlink\u0026#34;, \u0026#34;vdir\u0026#34;, \u0026#34;wc\u0026#34;, \u0026#34;whoami\u0026#34;, \u0026#34;yes\u0026#34; foreach ($e in $comands) { # enumerate over the whole array New-Item -Path (\u0026#34;${e}\u0026#34; + \u0026#34;.exe\u0026#34;) -ItemType SymbolicLink -Value coreutils.exe } Awesome Rust Modern Unix ","date":"2023-04-22T19:21:22+08:00","permalink":"https://www.lrh3321.win/p/pwsh-alias/","title":"在 Powershell 中增加命令别名"},{"content":" 本教程简单的描述了如何从零开始编写一个 Systemd 服务文件、SysV init 脚本到 Systemd Service 文件的转换、以及一些后续学习的资源类内容。关于 Systemd 服务的使用，请见管理 Systemd，服务的自动启动以及服务打包问题，请见Systemd 打包指南。\n什么是 Systemd service？ 一种以 .service 结尾的单元（unit）配置文件，用于控制由 systemd 控制或监视的进程。简单说，用于后台以守护精灵（daemon）的形式运行程序。\n编写 Systemd service 基本结构 Systemd 服务的内容主要分为三个部分，控制单元（unit）的定义、服务（service）的定义、以及安装部分。\n和 SysV init 脚本的差异 过去，*nix 服务（守护精灵）都是用 SysV 启动脚本启动的。SysV 启动脚本就是 Bash 脚本，通常在 /etc/init.d 目录下，可以被一些标准参数如 start，stop，restart 等调用。启动该脚本通常意味着启动一个后台守护精灵（daemon）。shell 脚本常见的缺点就是，慢、可读性不强、太详细又很傲娇。虽然它们很灵活（毕竟那就是代码呀），但是有些事只用脚本做还是显得太困难了，比如安排并列执行、正确监视进程，或者配置详细执行环境。\nSysV 启动脚本还有一个硬伤就是，臃肿，重复代码太多。因为上述的“标准参数”必须要靠各个脚本来实现，而且各个脚本之间的实现都差不多（根本就是从一个 skeleton 骨架来的）。而 Systemd 则进行了统一实现，也就是说在 Systemd service 中完全就不需要、也看不到这部分内容。这使得 Systemd 服务非常简明易读，例如 NetworkManager 这一重量级程序的服务，算上注释一共才有 19 行。而它相应的 SysV 启动脚本头 100 行连标准参数都没实现完。\nSystemd 兼容 Sysv 启动脚本，这也是为什么这么久我们仍然需要一个 systemd-sysvinit 软件包的原因。但是根据以上理由，最好针对所有您安装的守护精灵都使用原生 Systemd 服务来启动。另外，Systemd 服务可无缝用于所有使用 Systemd 的发行版，意思是 Arch 下编写的脚本拿过来依然能够使用。\n通常来说，上游应该在发布源代码的同时发布 Systemd 服务，但如果没发布，你可以对照本教学来为它们写一个并贡献给它们。\n关于 SysV init 启动脚本的编写可见SysVinit 引导脚本，这主要用于你的服务器，毕竟服务器追求稳定软件更新的不是很勤（但你一定不知道欧盟汽车里的车载系统必须是 Systemd）。\n真正开始前需要注意的问题 如上所述，Systemd 的 service 文件是完全跨发行版的，所以有时候没有必要重造轮子。真正编写你的服务前，请确认它在各大发行版中完全就不存在：\n我们的 Systemd 服务集合 Fedora Systemd 服务集合 Arch Linux Systemd 服务集合 Gentoo Systemd 服务集合 Debian 中的少量 Systemd 服务 ubuntu 中的少量 Systemd 服务 Systemd 语法 Systemd 语法和 .desktop 文件的语法比较像，也比较类似 Windows 下的 .ini 文件，因此无论对于打包者还是最终用户都是非常容易上手的。\n主要格式请见下面的小例子，这里需要说明三点：\nSystemd 单元文件中的以 # 开头的行后面的内容会被认为是注释\nSystemd 下的布尔值，1、yes、on、true 都是开启，0、no、off、false 都是关闭。注：\n仅限于 Systemd 文件，比如：\n1 RemainOnExit=yes 并不适用于该文件中嵌入的 shell 语句，比如：\n1 ExecStartPre=/usr/bin/test \u0026#34;x${NETWORKMANAGER}\u0026#34; = xyes 这里的 yes 就不能替换。因为等号后面是一条嵌入的 shell 语句。\nSystemd 下的时间单位默认是秒，所以要用毫秒（ms）分钟（m）等请显式说明。\n一个小例子 NetworkManager 的 Systemd service：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [Unit] Description=Network Manager After=syslog.target Wants=remote-fs.target network.target [Service] Type=dbus BusName=org.freedesktop.NetworkManager ExecStart=/usr/sbin/NetworkManager --no-daemon EnvironmentFile=/etc/sysconfig/network/config ExecStartPre=/usr/bin/test \u0026#34;x${NETWORKMANAGER}\u0026#34; = xyes # Suppress stderr to eliminate duplicated messages in syslog. NM calls openlog() # with LOG_PERROR when run in foreground. But systemd redirects stderr to # syslog by default, which results in logging each message twice. StandardError=null [Install] WantedBy=multi-user.target Also=NetworkManager-wait-online.service 以下我们以编写我们论坛所使用的 He.net IPv6 单元文件为例。\n定义控制单元 [Unit] 在 Systemd 中，所有引导过程中 Systemd 要控制的东西都是一个单元。Systemd 单元类型有：\n系统服务 套接字（socket） 设备 挂载点 自动挂载点 SWAP 文件 分区 启动对象（startup target） 文件系统路径 定时器 简单说，Systemd 把 *nix 里那些分散开发因此宏观看变成一团杂碎的东西重新统一命名了。单元名就是你写的这个 .service 文件的名称。但不只有 .service 后缀的文件才可以是一个单元，单元还可以有 .target, .path 等后缀，具体可以去 /usr/lib/systemd/system 下了解。但那种后缀要么由 Systemd 上游开发者写好随 systemd 软件包分发，要么由我们的 Base:system 团队添加，一般用户是不太需要写其它后缀的控制单元的。\n我们先要声明我们在定义控制单元：\n1 [Unit] 单元名称就不用写了，我们要写一条单元描述：\n1 2 [Unit] Description=Daemon to start He.net IPv6 下面我们要讲解一下 Systemd 是如何控制各个单元之间的关系的。它和 RPM 的 specfile 的依赖关系控制的语法非常相似（毕竟都是红帽一家的）：\nRequires: 这个单元启动了，那么它“需要”的单元也会被启动; 它“需要”的单元被停止了，它自己也活不了。但是请注意，这个设定并不能控制某单元与它“需要”的单元的启动顺序（启动顺序是另外控制的），即 Systemd 不是先启动 Requires 再启动本单元，而是在本单元被激活时，并行启动两者。于是会产生争分夺秒的问题，如果 Requires 先启动成功，那么皆大欢喜; 如果 Requires 启动得慢，那本单元就会失败（Systemd 没有自动重试）。所以为了系统的健壮性，不建议使用这个标记，而建议使用 Wants 标记。可以使用多个 Requires。 RequiresOverridable: 跟 Requires 很像。但是如果这条服务是由用户手动启动的，那么 RequiresOverridable 后面的服务即使启动不成功也不报错。跟 Requires 比增加了一定容错性，但是你要确定你的服务是有等待功能的。另外，如果不由用户手动启动而是随系统开机启动，那么依然会有 Requires 面临的问题。 Requisite: 强势版本的 Requires。要是这里需要的服务启动不成功，那本单元文件不管能不能检测等不能等待都立刻就会失败。 Wants: 推荐使用。本单元启动了，它“想要”的单元也会被启动。但是启动不成功，对本单元没有影响。 Conflicts: 一个单元的启动会停止与它“冲突”的单元，反之亦然。注意这里和后面的启动顺序是“正交”的： 两个相互冲突的单元被同时启动，要么两个都启动不了（两者都是第三个单元的 Requires），要么启动一个（有一个是第三个单元的 Requires，另一个不是），不是 Requires 的那个会被停止。要是两者都不是任何一个单元的 Requires，那么 Conflicts 别的那个单元优先启动，被 Conflicts 的后启动，要是互相写了，那么两个都启动不了。 OnFailure：很明显，如果本单元失败了，那么启动什么单元作为折衷。 好了，现在我们来想象一下，我们的单元（Ipv6 隧道）应该想要什么呢？很显然是一个连通着的网络。有一个 Systemd 默认提供的对象叫做 network-online.target（默认的 target 列表可见 systemd.special，必看，因为你大多数时候 Wants 的都是一个固定的系统状态而不是其它 systemd 服务），正正好好能够提供我们需要的环境。于是：\n1 2 3 [Unit] Description=Daemon to start He.net IPv6 Wants=network-online.target 下面我们需要定义一下服务启动顺序，不然连 / 目录所在的硬盘都没挂载就开始干活，上哪儿找程序去呀。Systemd 服务启动顺序主要使用以下两个标记定义的：\nBefore/After：要是一个服务 Before 另一个服务，那么在并行启动时（Systemd 总是用进程 0 并行启动所有东西，然后通过这两个标记来二次等待排序），那另一个服务这时就会等这个服务先启动并返回状态，注意是先启动而不是启动成功，因为失败也是一种状态，一定要成功才启动另一个服务是通过依赖关系定义的。反之 After 亦然。 下面说下“关机”（可以是挂起，这时候有些服务是依然在跑的，比如网络唤醒）时候的顺序：如果两个服务都是要关掉的，Before 是先关自己，After 是先关别人，这很好理解; 但如果一个服务是要关，而另一个是要开的，那么不管 Before/After 写了什么，总是优先关闭而不是开始。也就是比如服务 A Before 服务 B，但是服务 B 是在关，而服务 A 是在 restart，那么服务 B 的顺序在服务 A 的前面。\n好啦，我们的单元应该在什么的前后启动呢？它不需要一定在什么服务前面跑起来，这不像 ifup 和 dhcp，网络起不来获取 ip 肯定没用。我们只需要有网就可以了。“有网”在 Systemd 中也是由一个默认 target：network.target 提供的，于是我们的控制单元就定义好了：\n1 2 3 4 [Unit] Description=Daemon to start He.net IPv6 Wants=network-online.target After=network.target 定义服务本体 [service] 在定义完了 Systemd 用来识别服务的单元后，我们来定义服务本体，依然是声明：\n1 [Service] 然后是声明服务类型：\n1 2 [Service] Type= Systemd 支持的服务类型有以下几类：\nsimple 默认，这是最简单的服务类型。意思就是说启动的程序就是主体程序，这个程序要是退出那么一切皆休。这在图形界面里非常好理解，我打开 Amarok，退出它就没有了。但是命令行的大部分程序都不会那么设计，因为命令行的一个最基本原则就是一个好的程序不能独占命令行窗口。所以输入命令，回车，接着马上返回给你提示符，但程序已经执行了。所以只有少数程序比如 python xxx.py 还使用这种方式。在这种类型下面，如果你的主程序是要响应其它程序的，那么你的通信频道应该在启动本服务前就设好（套接字等），因此这种类型的服务，Systemd 运行它后会立刻就运行下面的服务（需要它的服务），这时没有套接字后面的服务会失败，写 After 也没用，因为 simple 类型不存在主进程退出的情况也就不存在有返回状态的情况，所以它一旦启动就认为是成功的，除非没起来。 forking 标准 Unix Daemon 使用的启动方式。启动程序后会调用 fork() 函数，把必要的通信频道都设置好之后父进程退出，留下守护精灵的子进程。你要是使用的这种方式，最好也指定下 PIDFILE=，不要让 Systemd 去猜，非要猜也可以，把 GuessMainPID 设为 yes。 判断是 forking 还是 simple 类型非常简单，命令行里运行下你的程序，持续占用命令行要按 Ctrl + C 才可以的，就不会是 forking 类型。\n创建 PIDFILE 是你为它写服务的程序的任务而不是 Systemd 的功能，甚至也不是 Sysvinit 脚本的功能。参考 startproc创建pid file的问题了解进一步的知识。因此如果你的程序确实是 forking 类型，但就是没实现创建 PIDFILE 的功能，那么建议使用 ExecStartPost= 结合 shell 命令来手动抓取进程编号并写到 /var/run/xxx.pid。\noneshot 顾名思义，打一枪换一个地方。所以这种服务类型就是启动，完成，没进程了。常见比如你设置网络，ifup eth0 up，就是一次性的，不存在 ifup 的子进程（forking 那样），也不存在主进程（simple 那样），它运行完成后便了无痕迹。因为这类服务运行完就没进程了，我们经常会需要 RemainAfterExit=yes。后面配置的意思是说，即使没进程了，我们也要 Systemd 认为该服务是存在并成功了的。所以如果你有一个这样的服务，服务启动后，你再去 ifup eth0 up，这时你再看服务，依然显示是 running 的。因为只要在执行那条一次性命令的时候没出错，那么它就永远认为它是成功并一直存在的，直到你关闭服务。 dbus 这个程序启动时需要获取一块 DBus 空间，所以需要和 BusName= 一起用。只有它成功获得了 DBus 空间，依赖它的程序才会被启动。 一般人也就能用到上面四个，还有两种少见的类型：\nnotify 这个程序在启动完成后会通过 sd_notify 发送一个通知消息。所以还需要配合 NotifyAccess 来让 Systemd 接收消息，后者有三个级别：none，所有消息都忽略掉; main，只接受我们程序的主进程发过去的消息; all，我们程序的所有进程发过去的消息都算。NotifyAccess 要是不写的话默认是 main。 idle 这个程序要等它里面调度的全部其它东西都跑完才会跑它自己。比如你 ExecStart 的是个 shell 脚本，里面可能跑了一些别的东西，如果不这样的话，那很可能别的东西的控制台输出里会多一个“启动成功”这样的 Systemd 消息。 由于 He.net 的 IPv6 是用 iproute2 的 ip 命令来弄的，所以是一个 oneshot 一次性服务。\n1 2 3 [Service] Type=oneshot RemainAfterExit=yes 接下来要设置 ExecStart, ExecStop。如果程序支持的话，你还可以去设置 ExecReload，Restart 等。注意，这里设置的是它们 Reload/Restart 的方式，但并不代表没有它们 Systemd 就不能完成比如 systemctl restart xxx.service 这样的任务，程序有支持自然最好，程序不支持那就先 stop 再 start 咯。同样有特殊要求的时候你也可以去设置比如 ExecStartPre/ExecStartPost,RestartSec,TimeoutSec 等其它东西，参考链接里都有使用方法。\n这里要特殊讲一下 ExecStart：\n如果你服务的类型不是 oneshot，那么它只可以接受一个命令，参数不限，比如你先 ip tunnel create 再 ip tunnel0 up，那是两个 ip 命令，如果你不是 oneshot 类型这样是不行的。 如果有多条命令（oneshot 类型），命令之间以分号 ; 分隔，跨行可用反斜杠 \\。 除非你的服务类型是 forking，否则你在这里输入的命令都会被认为是主进程，不管它是不是。 于是我们的 [Service] 就写好了：\n1 2 3 4 5 6 7 8 9 10 11 12 [Service] Type=oneshot RemainAfterExit=yes ExecStart=/usr/sbin/ip tunnel add he-ipv6 mode sit remote 66.220.18.42 local 108.170.7.158 ttl 255 ; \\ /usr/sbin/ip link set he-ipv6 up ; \\ /usr/sbin/ip addr add 2001:470:c:1184::2/64 dev he-ipv6 ; \\ /usr/sbin/ip route add ::/0 dev he-ipv6 ; \\ /usr/sbin/ip -6 addr ExecStop=/usr/sbin/ip route delete ::/0 dev he-ipv6 ; \\ /usr/sbin/ip -6 addr del 2001:470:c:1184::2/64 dev he-ipv6 ; \\ /usr/sbin/ip link set he-ipv6 down ; \\ /usr/sbin/ip tunnel del he-ipv6 安装服务 [install] 这可能有点绕，我服务文件都弄好了，放到 /etc/systemd/system（供系统管理员和用户使用），/usr/lib/systemd/system（供发行版打包者使用）了，不就是安装好了嘛。\n这里说的是一种内部状态，默认你放对位置它显示的是 disabled，unloaded，所以我们要在 Systemd 内部对它进行一下 load，没人要的东西是不需要安装的（我们不收渣渣），所以我们要告诉 Systemd 它是有人要的，被谁要。一般都是被\n1 2 [Install] WantedBy=multi-user.target 要（multi-user.target 表示多用户系统好了，简单理解就是你可以登入了）。这样在 multi-user.target 启用时，我们的服务也就会被启用了。\n[Install] 部分下除了 WantedBy 还有两种属性，分别是：\nAlias= 给你自己的别名，这样 systemctl command xxx.service 的时候就可以不输入完整的单元名称。比如你给 NetworkManager 一个别名叫 Alias=nm，那你就可以 systemctl status nm.service 查看实际是 NetworkManager.service 的服务了。 Also= 安装本服务的时候还要安装别的什么服务。比如我们的 He.net 脚本按理应该需要一个 iproute2.service 作为 also，但是 iproute2 实际上不需要 systemd 控制，所以就没写。它和 [Unit] 定义里面的依赖关系相比，它管理的不是运行时依赖，而是安装时。安装好了之后启动谁先谁后，谁依赖谁，和 Also= 都没有关系。 参考文献 systemd.service 手册页 systemd.unit 手册页 systemd.special 手册页 Systemd 作者 Lennart 博客连载的“给系统管理员的 Systemd 教学之三” 作者 MargueriteSu\nopenSUSE:How to write a systemd service systemd - ArchWiki systemd/Timers - ArchWiki ","date":"2023-04-16T08:37:29+08:00","permalink":"https://www.lrh3321.win/p/how-to-write-a-systemd-service/","title":"如何编写 Systemd 服务文件"},{"content":"Git 使用小技巧 取消 sslVerify 应对 SNI 阻断\ngit config --global http.sslVerify false\n使用 SSH 替代 HTTP 方式拉取 Repo 尤其适合 Golang 从私有仓库拉取包\n~/.gitconfig\n1 2 3 4 [url \u0026#34;git@github.com:\u0026#34;] insteadOf = https://github.com/ [url \u0026#34;git@gitlab.com:\u0026#34;] insteadOf = https://gitlab.com/ 1 2 3 4 5 6 7 8 # 或者： git config --global url.\u0026#34;git@gitlab.com:groupName/projectName.git\u0026#34;.insteadOf \u0026#34;https://gitlab.com/groupName/projectName.git\u0026#34;` # 全局替换,拉取域名下的所有包 git config --global url.\u0026#34;git@gitlab.yoursite.com:\u0026#34;.insteadof \u0026#34;https://gitlab.yoursite.com/\u0026#34; git config --global url.\u0026#34;https://\u0026#34;.insteadOf git:// Git 跟踪文件更改 1 git log --follow --patch routes/admin/contract_operation.js 使用 HTTP/1.1 1 git --global config http.version HTTP/1.1 使用gitlab作为go mod私服 git-config github-git-cheat-sheet ","date":"2023-04-15T19:23:03+08:00","permalink":"https://www.lrh3321.win/p/git-tips/","title":"Git Tips"},{"content":"tcpdump 捕获 netlink 包 NLMON\nNLMON is a Netlink monitor device.\nUse an NLMON device when you want to monitor system Netlink messages.\nHere\u0026rsquo;s how to create an NLMON device:\n1 2 3 ip link add nlmon0 type nlmon ip link set nlmon0 up tcpdump -i nlmon0 -w nlmsg.pcap This creates an NLMON device named nlmon0 and sets it up. Use a packet sniffer (for example, tcpdump) to capture Netlink messages. Recent versions of Wireshark feature decoding of Netlink messages.\n1 2 3 4 5 6 7 modprobe nlmon ip link add type nlmon ip link set nlmon0 up tcpdump -i nlmon0 -s 0 -w nlmon.pcap ip link set nlmon0 down ip link del dev nlmon0 rmmod nlmon 1 2 3 4 5 6 modprobe nlmon ip netns add testing ip -n testing link add nlmon0 type nlmon ip -n testing link set dev nlmon0 up # 必须使用 -w 保存到文件 ip netns exec tcpdump -i nlmon0 -w netlinik.pcap 参考链接 Introduction to Linux interfaces for virtual networking ","date":"2021-07-17T11:44:17+08:00","permalink":"https://www.lrh3321.win/p/tcpdump-capture-netlink-packets/","title":"tcpdump 捕获 netlink 包"},{"content":"复用已建立的 SSH 连接，减少输入密码次数 有时候登录ssh机器是一个动态口令，频繁输入密码特别不方便。可以通过连接复用的方式来达到不用每次输入密码的目的。\nMac上没有诸如xshell之类的客户端，对于非免密登录的主机来说，多个相同主机的连接就要输入多次密码，无法实现会话克隆，非常难受。 终端的ssh是标准的OpenSSH client 如果需要克隆会话功能，可以通过配置打开:\n1 vi ~/.ssh/config 增加：\n1 2 3 4 Host * ControlMaster auto ControlPath ~/.ssh/socket/%h-%p-%r ControlPersist yes 这样每连上一个服务器都会自动在~/.ssh/socket/下创建一个socket文件，下次用相同用户名、端口、主机名进行连接就会自动复用，为防止意外，请事先使用对应用户创建好该目录\nControlMaster 默认是关闭的，通过上述配置可以打开,我们可以通过ControlMaster字段，让新建的SSH Session复用已有的socket通信文件。当我们将该字段值设置为auto时，每次建立SSH连接时程序都会检查是否存在已有到socket文件，有即复用，没有的话就创建一个符合ControlPath规则的socket文件。\nControlPath 用来描述socket文件路径，其中：\n%r 是用户名， %h 是远程主机 IP %p 是端口\nControlPersist yes 打开之后即使关闭了所有的已连接ssh连接，一段时间内也能无需密码重新连接。也可以写作： ControlPersist 4h 每次通过SSH与服务器建立连接之后，这条连接将被保持4个小时，即使在你退出服务器之后这条连接依然可以重用，因此，在你下一次(4小时之内)登录服务器时，你会发现连接以闪电般的速度建立完成，这个选项对于通过scp拷贝多个文件提速尤其明显，因为你不在需要为每个文件做单独的认证了\n注：SSH版本必须是5.6或以上版本才可使用ControlPersist特性 配置文件最终如下：\n1 2 3 4 5 6 7 8 Host * Compression yes ServerAliveInterval 30 ServerAliveCountMax 360 ControlMaster auto ControlPath ~/.ssh/socket/%h-%p-%r ControlPersist yes ForwardAgent yes 参考链接 Mac/Linux下SSH管理 开启 ControlPersist 来大幅度提升 SSH 的连接速度 ","date":"2021-06-17T11:44:17+08:00","permalink":"https://www.lrh3321.win/p/%E5%A4%8D%E7%94%A8%E5%B7%B2%E5%BB%BA%E7%AB%8B%E7%9A%84-ssh-%E8%BF%9E%E6%8E%A5%E5%87%8F%E5%B0%91%E8%BE%93%E5%85%A5%E5%AF%86%E7%A0%81%E6%AC%A1%E6%95%B0/","title":"复用已建立的 SSH 连接，减少输入密码次数"},{"content":"为 Flutter 设定镜像配置 macOS / Linux 1 2 3 4 5 6 export PUB_HOSTED_URL=https://pub.flutter-io.cn export FLUTTER_STORAGE_BASE_URL=https://storage.flutter-io.cn git clone -b stable https://github.com/flutter/flutter.git export PATH=\u0026#34;$PWD/flutter/bin:$PATH\u0026#34; cd ./flutter flutter doctor Windows 1 2 3 4 5 6 7 8 9 $env:PUB_HOSTED_URL=\u0026#34;https://pub.flutter-io.cn\u0026#34; $env:FLUTTER_STORAGE_BASE_URL=\u0026#34;https://storage.flutter-io.cn\u0026#34; git clone -b stable https://github.com/flutter/flutter.git $env:PATH=\u0026#34;$PWD\\flutter\\bin:$env:PATH\u0026#34; cd ./flutter flutter doctor [environment]::SetEnvironmentvariable(\u0026#34;PUB_HOSTED_URL\u0026#34;, \u0026#34;https://pub.flutter-io.cn\u0026#34;, \u0026#34;User\u0026#34;) [environment]::SetEnvironmentvariable(\u0026#34;FLUTTER_STORAGE_BASE_URL\u0026#34;, \u0026#34;https://storage.flutter-io.cn\u0026#34;, \u0026#34;User\u0026#34;) 社区运行的镜像站点 Flutter 社区 社区主镜像，采用多种方式同步 Flutter 开发者资源（推荐）。\n1 2 export PUB_HOSTED_URL=https://pub.flutter-io.cn export FLUTTER_STORAGE_BASE_URL=https://storage.flutter-io.cn 上海交大 Linux 用户组 使用反向代理方式建立 Flutter 镜像，数据与站源实时同步。 Pub API 返回值未做处理，可能造成无法访问的情况。\n1 2 export PUB_HOSTED_URL=https://dart-pub.mirrors.sjtug.sjtu.edu.cn export FLUTTER_STORAGE_BASE_URL=https://mirrors.sjtug.sjtu.edu.cn 清华大学 TUNA 协会 采取自定义脚本定时主动抓取策略，并配置了完善的回源策略（推荐）。查看帮助文档： Flutter 镜像安装帮助， Pub 镜像安装帮助。\n1 2 export PUB_HOSTED_URL=https://mirrors.tuna.tsinghua.edu.cn/dart-pub export FLUTTER_STORAGE_BASE_URL=https://mirrors.tuna.tsinghua.edu.cn/flutter CNNIC 基于 TUNA 协会的镜像服务，数据策略与 TUNA 一致，通过非教育网的域名访问（建议选择 TUNA）。\n1 2 export PUB_HOSTED_URL=http://mirrors.cnnic.cn/dart-pub export FLUTTER_STORAGE_BASE_URL=http://mirrors.cnnic.cn/flutter 腾讯云开源镜像站 使用 TUNA 开源的脚本每天凌晨 0 - 2 点定时与站源同步，未配置回源策略。\n1 2 export PUB_HOSTED_URL=https://mirrors.cloud.tencent.com/dart-pub export FLUTTER_STORAGE_BASE_URL=https://mirrors.cloud.tencent.com/flutter 参考 Using Flutter in China 在中国网络环境下使用 Flutter Flutter 镜像安装帮助 Windows 10 环境变量：如何通过 CMD 和 PowerShell 写入环境变量 ","date":"2020-11-21T13:50:44+08:00","permalink":"https://www.lrh3321.win/p/%E4%B8%BA-flutter-%E8%AE%BE%E5%AE%9A%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE/","title":"为 Flutter 设定镜像配置"},{"content":"挂载 Raw 和 QCOW2 格式镜像 挂载 Raw 格式镜像 Mounting The Raw Image Associate the raw image with a loop device:\n1 losetup /dev/loop0 image.raw Map the partitions to loop devices:\n1 kpartx -a /dev/loop0 You should be able to mount the partitions now:\n1 mount /dev/mapper/loop0p1 /mnt/t01 Unmounting The Raw Image Unmount the previously mounted partitions:\n1 umount /dev/t01 Undo the mapping of the partitions to the loop devices:\n1 kpartx -d /dev/loop0 Destroy the loop:\n1 losetup -d /dev/loop0 挂载 QCOW2 格式镜像 This is a quick guide to mounting a qcow2 disk images on your host server. This is useful to reset passwords, edit files, or recover something without the virtual machine running.\nStep 1 - Enable NBD on the Host 1 2 3 if [ \u0026#34;$(lsmod | grep -E \u0026#39;\\bnbd\\s\u0026#39; -c)\u0026#34; -lt 1 ]; then modprobe nbd max_part=8 fi Step 2 - Connect the QCOW2 as network block device 1 2 qemu-nbd --connect=/dev/nbd0 vm-100-disk-1.qcow2 # qemu-nbd -c /dev/nbd0 vm-100-disk-1.qcow2 Step 3 - Find The Virtual Machine Partitions 1 2 # fdisk /dev/nbd0 -l fdisk /dev/nbd0 -l|grep -E \u0026#39;/dev/.*p[0-9]\u0026#39;|cut -d\u0026#39; \u0026#39; -f1 Step 4 - Mount the partition from the VM 1 2 mkdir -p /mnt/somepoint/ mount /dev/nbd0p1 /mnt/somepoint/ Step 5 - After you done, unmount and disconnect 1 2 3 4 umount /mnt/somepoint/ qemu-nbd --disconnect /dev/nbd0 # qemu-nbd -d /dev/nbd0 rmmod nbd Mounting Raw and qcow2 Images How to mount a qcow2 disk image ","date":"2020-07-17T15:26:22+08:00","permalink":"https://www.lrh3321.win/p/mounting-raw-and-qcow2-images/","title":"Mounting Raw and QCOW2 Images"},{"content":"Windows 无限试用 JetBrains 系列产品 2020 系列 删除 %APPDATA%\\JetBrains\\\u0026lt;产品名\u0026gt;\\eval\\*.key，如 GoLand 2020.1.2 产品就是 GoLand2020.1\nPowerShell 脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 修改为系统上安装了的版本 $products = \u0026#34;IntelliJIdea2020.1\u0026#34;, \u0026#34;GoLand2020.1\u0026#34;, \u0026#34;PyCharm2020.1\u0026#34;, \u0026#34;WebStorm2020.1\u0026#34; foreach ($product in $products) { $parent = \u0026#34;${ENV:APPDATA}\\JetBrains\\${product}\\eval\u0026#34; if (Test-Path -Path $parent) { Write-Host -ForegroundColor Green \u0026#34;Remove:\u0026#34; \u0026#34;$parent\\*.key\u0026#34; Remove-Item \u0026#34;$parent\\*.key\u0026#34; } else { Write-Host -ForegroundColor Yellow $parent \u0026#34;Not Found\u0026#34; } } 2019 以及之前版本 删除 %USERPROFILE%\\\u0026lt;.产品名\u0026gt;\\config\\eval\\*.key 如 GoLand 2019.3.1 产品就是 .GoLand2019.3\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 修改为系统上安装了的版本 $products = \u0026#34;GoLand2019.3\u0026#34;, \u0026#34;PyCharm2019.3\u0026#34;, \u0026#34;WebStorm2019.3\u0026#34; foreach ($product in $products) { $parent = \u0026#34;${ENV:USERPROFILE}\\.${product}\\config\\eval\u0026#34; if (Test-Path -Path $parent) { Write-Host -ForegroundColor Green \u0026#34;Remove:\u0026#34; \u0026#34;$parent\\*.key\u0026#34; Remove-Item \u0026#34;$parent\\*.key\u0026#34; } else { Write-Host -ForegroundColor Yellow $parent \u0026#34;Not Found\u0026#34; } } 参考 关于无限试用JetBrains产品的方案 ","date":"2020-06-24T10:14:39+08:00","permalink":"https://www.lrh3321.win/p/windows-%E6%97%A0%E9%99%90%E8%AF%95%E7%94%A8-jetbrains-%E7%B3%BB%E5%88%97%E4%BA%A7%E5%93%81/","title":"Windows 无限试用 JetBrains 系列产品"},{"content":"NodeJS 使用阿里镜像仓库 1 2 3 4 5 6 7 8 9 10 11 npm config set registry https://registry.npmmirror.com/ npm config set disturl https://npmmirror.com/mirrors/node/ npm config set sass_binary_site https://npmmirror.com/mirrors/node-sass/ npm config set sharp_dist_base_url https://npmmirror.com/mirrors/sharp-libvips/ npm config set electron_mirror https://npmmirror.com/mirrors/electron/ npm config set puppeteer_download_host https://npmmirror.com/mirrors/ npm config set phantomjs_cdnurl https://npmmirror.com/mirrors/phantomjs/ npm config set sentrycli_cdnurl https://npmmirror.com/mirrors/sentry-cli/ npm config set sqlite3_binary_site https://npmmirror.com/mirrors/sqlite3/ npm config set python_mirror https://npmmirror.com/mirrors/python/ 1 yarn config set registry https://registry.npmmirror.com/ ","date":"2020-06-19T21:52:43+08:00","permalink":"https://www.lrh3321.win/p/%E8%AE%BE%E7%BD%AE-npm-%E9%95%9C%E5%83%8F%E6%BA%90/","title":"设置 npm 镜像源"},{"content":"为终端设置代理 linux shell 终端代理设置方法 linux 要在 shell 终端为 http https ftp 协议设置代理，值需要设置对应的环境变量即可。\n下面是一些关于代理的环境变量：\n环境变量 描述 值示例 http_proxy 为http网站设置代理 10.0.0.51:8080 user:pass@10.0.0.10:8080 socks4://10.0.0.51:1080 socks5://192.168.1.1:1080 https_proxy 为https网站设置代理 同上 ftp_proxy 为ftp协议设置代理 socks5://192.168.1.1:1080 no_proxy 无需代理的主机或域名，可以使用通配符，多个时使用 , 号分隔 *.aiezu.com,10.*.*.*,192.168.*.*,*.local,localhost,127.0.0.1 可以将上面4个环境变量设置项放于 ~/.bashrc 文件尾部，这样用户打开bash shell终端时会自动调用此脚本，读入它们。\nbash http 站点设置代理 根据代理类型，将下面对应的设置项添加到 ~/.bashrc 文件末尾，然后运行 . ~/.bashrc 命令使用之在当前环境生效。\n1 2 3 4 5 6 7 8 9 10 11 # 为http站点设置http代理（默认）： export http_proxy=10.0.0.52:8080 # 为http站点设置sock4、sock5代理： # 设置 socks 代理，自动识别socks版本 export http_proxy=socks://10.0.0.52:1080 # 设置 socks4 代理 export http_proxy=socks4://10.0.0.52:1080 # 设置 socks5 代理 export http_proxy=socks5://10.0.0.52:1080 # 代理使用用户名密码认证： export http_proxy=user:pass@192.158.8.8:8080 bash https 站点设置代理 如果需要为https网站设置代理，设置https_proxy环境变量即可；设置方法完全与http_proxy环境变量相同：\n1 2 3 4 5 6 # 任意使用一项 export https_proxy=10.0.0.52:8080 export https_proxy=user:pass@192.158.8.8:8080 export https_proxy=socks://10.0.0.52:1080 export https_proxy=socks4://10.0.0.52:1080 export https_proxy=socks5://10.0.0.52:1080 单独为 Git 启用代理 HTTP 代理 1 2 3 4 5 6 # 全局 git config --global http.proxy http://127.0.0.1:1089 git config --global https.proxy http://127.0.0.1:1089 # 仓库 git config http.proxy http://127.0.0.1:1089 git config https.proxy http://127.0.0.1:1089 Socks 5 代理 1 2 3 4 5 6 # 全局 git config --global http.proxy socks5://127.0.0.1:1088 git config --global https.proxy socks5://127.0.0.1:1088 # 仓库 git config http.proxy socks5://127.0.0.1:1088 git config https.proxy socks5://127.0.0.1:1088 取消代理 1 2 3 4 5 6 # 全局 git config --global --unset http.proxy git config --global --unset https.proxy # 仓库 git config --unset http.proxy git config --unset https.proxy 取消 SSL Verify 1 git config --global http.sslVerify false 参考链接 Linux bash终端设置代理（proxy）访问 ","date":"2020-04-13T14:16:53+08:00","permalink":"https://www.lrh3321.win/p/proxy-for-terminal/","title":"为终端设置代理"},{"content":"Redis 常见面试题 什么是 Redis ? Redis 是一个基于内存的高性能key-value数据库。\nRedis 的特点　Redis 本质上是一个 Key-Value 类型的内存数据库，很像 Memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作， Redis 的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。\nRedis 的出色之处不仅仅是性能，Redis 最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 Memcached只能保存1MB的数据，因此 Redis 可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外 Redis 也可以对存入的Key-Value设置expire时间，因此也可以被当作一个功能加强版的Memcached来用。\nRedis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此 Redis 适合的场景主要局限在较小数据量的高性能操作和运算上。\n使用 Redis 有哪些好处？ 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是 O(1) 支持丰富数据类型，支持string list set sorted set hash stream 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 Redis 相比 Memcached有哪些优势？ Memcached所有的值均是简单的字符串， Redis 作为其替代者，支持更为丰富的数据类型 Redis的速度比Memcached快很多 Redis可以持久化其数据 Memcache 与 Redis 的区别都有哪些？ 存储方式 Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，这样能保证数据的持久性。 数据支持类型 Memcache对数据类型支持相对简单。 Redis有复杂的数据类型。 使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 Redis 常见性能问题和解决方案： Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以Master最好不要写内存快照。 Master AOF持久化，如果不重写AOF文件，这个持久化方式对性能的影响是最小的，但是AOF文件会不断增大，AOF文件过大会影响Master重启的恢复速度。Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化,如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。 Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。 Redis主从复制的性能问题，为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内 为什么 Redis 需要把所有数据放到内存中?　Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以 Redis 具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响 Redis 的性能。在内存越来越便宜的今天， Redis 将会越来越受欢迎。\n如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。\nRedis 是单进程单线程的 Redis 利用队列技术将并发访问变为串行访问，消除了传统数据库串行控制的开销\nRedis 的并发竞争问题如何解决? Redis 为单进程单线程模式，采用队列模式将并发访问变为串行访问。 Redis 本身没有锁的概念， Redis 对于多个客户端连接并不存在竞争，但是在Jedis客户端对 Redis 进行并发访问时会发生连接超时、数据转换错误、阻塞、客户端关闭连接等问题，这些问题均是由于客户端连接混乱造成。对此有2种解决方法：\n客户端角度，为保证每个客户端间正常有序与 Redis 进行通信，对连接进行池化，同时对客户端读写 Redis 操作采用内部锁synchronized。 服务器角度，利用setnx实现锁。 注：对于第一种，需要应用程序自己处理资源的同步，可以使用的方法比较通俗，可以使用synchronized也可以使用lock；第二种需要用到 Redis 的setnx命令，但是需要注意一些问题。\nRedis 事物的了解CAS(check-and-set 操作实现乐观锁)? 和众多其它数据库一样， Redis 作为NoSQL数据库也同样提供了事务机制。在 Redis 中，MULTI/EXEC/DISCARD/WATCH这四个命令是我们实现事务的基石。相信对有关系型数据库开发经验的开发者而言这一概念并不陌生，即便如此，我们还是会简要的列出 Redis 中事务的实现特征：\n在事务中的所有命令都将会被串行化的顺序执行，事务执行期间， Redis 不会再为其它客户端的请求提供任何服务，从而保证了事物中的所有命令被原子的执行。 和关系型数据库中的事务相比，在 Redis 事务中如果有某一条命令执行失败，其后的命令仍然会被继续执行。 我们可以通过MULTI命令开启一个事务，有关系型数据库开发经验的人可以将其理解为\u0026quot;BEGIN TRANSACTION\u0026quot;语句。在该语句之后执行的命令都将被视为事务之内的操作，最后我们可以通过执行EXEC/DISCARD命令来提交/回滚该事务内的所有操作。这两个 Redis 命令可被视为等同于关系型数据库中的COMMIT/ROLLBACK语句。 在事务开启之前，如果客户端与服务器之间出现通讯故障并导致网络断开，其后所有待执行的语句都将不会被服务器执行。然而如果网络中断事件是发生在客户端执行EXEC命令之后，那么该事务中的所有命令都会被服务器执行。 当使用Append-Only模式时， Redis 会通过调用系统函数write将该事务内的所有写操作在本次调用中全部写入磁盘。然而如果在写入的过程中出现系统崩溃，如电源故障导致的宕机，那么此时也许只有部分数据被写入到磁盘，而另外一部分数据却已经丢失。 Redis服务器会在重新启动时执行一系列必要的一致性检测，一旦发现类似问题，就会立即退出并给出相应的错误提示。此时，我们就要充分利用 Redis 工具包中提供的 Redis -check-aof工具，该工具可以帮助我们定位到数据不一致的错误，并将已经写入的部分数据进行回滚。修复之后我们就可以再次重新启动 Redis 服务器了。\nWATCH命令和基于CAS的乐观锁：　在 Redis 的事务中，WATCH命令可用于提供CAS(check-and-set)功能。假设我们通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Null multi-bulk应答以通知调用者事务执行失败。例如，我们再次假设 Redis 中并未提供incr命令来完成键值的原子性递增，如果要实现该功能，我们只能自行编写相应的代码。其伪码如下：\n1 2 3 val = GET mykey val = val + 1 SET mykey $val 以上代码只有在单连接的情况下才可以保证执行结果是正确的，因为如果在同一时刻有多个客户端在同时执行该段代码，那么就会出现多线程程序中经常出现的一种错误场景\u0026ndash;竞态争用(race condition)。比如，客户端A和B都在同一时刻读取了mykey的原有值，假设该值为10，此后两个客户端又均将该值加一后set回 Redis 服务器，这样就会导致mykey的结果为11，而不是我们认为的12。为了解决类似的问题，我们需要借助WATCH命令的帮助，见如下代码：\n1 2 3 4 5 6 WATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。\nRedis 持久化的几种方式 快照（snapshots） 缺省情况情况下， Redis 把数据快照存放在磁盘上的二进制文件中，文件名为dump.rdb。你可以配置 Redis 的持久化策略，例如数据集中每N秒钟有超过M次更新，就将数据写入磁盘；或者你可以手工调用命令SAVE或BGSAVE。\n工作原理\nRedis forks. 子进程开始将数据写到临时RDB文件中。 当子进程完成写RDB文件，用新文件替换老文件。 这种方式可以使 Redis 使用copy-on-write技术。 AOF 快照模式并不十分健壮，当系统停止，或者无意中 Redis 被kill掉，最后写入 Redis 的数据就会丢失。这对某些应用也许不是大问题，但对于要求高可靠性的应用来说， Redis 就不是一个合适的选择。Append-only文件模式是另一种选择。你可以在配置文件中打开AOF模式\n虚拟内存方式 当你的key很小而value很大时,使用VM的效果会比较好.因为这样节约的内存比较大. 当你的key不小时,可以考虑使用一些非常方法将很大的key变成很大的value,比如你可以考虑将key,value组合成一个新的value. vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.\n自己测试的时候发现用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库\nRedis 的缓存失效策略和主键失效机制 作为缓存系统都要定期清理无效数据，就需要一个主键失效和淘汰策略.\n在 Redis 当中，有生存期的key被称为volatile。在创建缓存时，要为给定的key设置生存期，当key过期的时候（生存期为0），它可能会被删除。\n影响生存时间的一些操作 生存时间可以通过使用 DEL 命令来删除整个 key 来移除，或者被 SET 和 GETSET 命令覆盖原来的数据，也就是说，修改key对应的value和使用另外相同的key和value来覆盖以后，当前数据的生存时间不同。\n比如说，对一个 key 执行INCR命令，对一个列表进行LPUSH命令，或者对一个哈希表执行HSET命令，这类操作都不会修改 key 本身的生存时间。另一方面，如果使用RENAME对一个 key 进行改名，那么改名后的 key的生存时间和改名前一样。 RENAME命令的另一种可能是，尝试将一个带生存时间的 key 改名成另一个带生存时间的 another_key ，这时旧的 another_key (以及它的生存时间)会被删除，然后旧的 key 会改名为 another_key ，因此，新的 another_key 的生存时间也和原本的 key 一样。使用PERSIST命令可以在不删除 key 的情况下，移除 key 的生存时间，让 key 重新成为一个persistent key 。\n如何更新生存时间 可以对一个已经带有生存时间的 key 执行 EXPIRE命令，新指定的生存时间会取代旧的生存时间。过期时间的精度已经被控制在1ms之内，主键失效的时间复杂度是 O(1)，EXPIRE和TTL命令搭配使用，TTL可以查看key的当前生存时间。设置成功返回 1；当 key 不存在或者不能为 key 设置生存时间时，返回 0 。\n最大缓存配置在 Redis 中，允许用户设置最大使用内存大小 server.maxmemory 默认为0，没有指定最大缓存，如果有新的数据添加，超过最大内存，则会使 Redis 崩溃，所以一定要设置。Redis 内存数据集大小上升到一定大小的时候，就会实行数据淘汰策略。 Redis 提供 6种数据淘汰策略：\nvolatile-lru: 从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl: 从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random: 从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru: 从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random: 从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction: 禁止驱逐数据 注意这里的6种机制，volatile和allkeys规定了是对已设置过期时间的数据集淘汰数据还是从全部数据集淘汰数据，后面的lru、ttl以及random是三种不同的淘汰策略，再加上一种no-enviction永不回收的策略。\n使用策略规则：\n如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用allkeys-lru 如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用allkeys-random 三种数据淘汰策略：\nttl和random比较容易理解，实现也会比较简单。主要是lru最近最少使用淘汰策略，设计上会对key 按失效时间排序，然后取最先失效的key进行淘汰\nRedis 最适合的场景 Redis 最适合所有数据in-momory的场景，虽然 Redis 也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎 Redis 更像一个加强版的Memcached，那么何时使用Memcached,何时使用 Redis 呢?\n如果简单地比较 Redis 与Memcached的区别，大多数都会得到以下观点：\nRedis 不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis 支持数据的备份，即master-slave模式的数据备份。 Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 会话缓存（Session Cache） 最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他存储（如Memcached）的优势在于： Redis 提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？\n幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台Magento也提供 Redis 的插件。\n全页缓存（FPC） 除基本的会话token之外， Redis 还提供很简便的FPC平台。回到一致性问题，即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。\n再次以Magento为例，Magento提供一个插件来使用 Redis 作为全页缓存后端。\n此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp- Redis ，这个插件能帮助你以最快速度加载你曾浏览过的页面。\n队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。 Redis 作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。\n如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用 Redis 作为broker，你可以从这里去查看。\n排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单， Redis 只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们\n称之为“user_scores”，我们只需要像下面一样执行即可：\n当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：\n1 ZRANGE user_scores 0 10 WITHSCORES Agora Games就是一个很好的例子，用Ruby实现的，它的排行榜就是使用 Redis 来存储数据的，你可以在这里看到。\n发布/订阅 最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用 Redis 的发布/订阅功能来建立聊天系统！（不，这是真的，你可以去核实）。\nReference Redis 的那些最常见面试问题 几率大的 Redis 面试题 ","date":"2020-02-28T16:14:32+08:00","permalink":"https://www.lrh3321.win/p/redis-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"Redis 常见面试题"},{"content":"SSH 端口转发 SSH 会自动加密和解密所有 SSH 客户端与服务端之间的网络数据。但是，SSH 还同时提供了一个非常有用的功能，这就是端口转发。它能够将其他 TCP 端口的网络数据通过 SSH 链接来转发，并且自动提供了相应的加密及解密服务。这一过程有时也被叫做“隧道”（tunneling），这是因为 SSH 为其他 TCP 链接提供了一个安全的通道来进行传输而得名。例如，Telnet，SMTP，LDAP 这些 TCP 应用均能够从中得益，避免了用户名，密码以及隐私信息的明文传输。而与此同时，如果您工作环境中的防火墙限制了一些网络端口的使用，但是允许 SSH 的连接，那么也是能够通过将 TCP 端口转发来使用 SSH 进行通讯。总的来说 SSH 端口转发能够提供两大功能：\n加密 SSH Client 端至 SSH Server 端之间的通讯数据。 突破防火墙的限制完成一些之前无法建立的 TCP 连接。 使用了端口转发之后，TCP 端口 A 与 B 之间现在并不直接通讯，而是转发到了 SSH 客户端及服务端来通讯，从而自动实现了数据加密并同时绕过了防火墙的限制。\nSSH端口本地转发 假设在实验室里有一台 LDAP 服务器（LdapServerHost），但是限制了只有本机上部署的应用才能直接连接此 LDAP 服务器。如果我们由于调试或者测试的需要想临时从远程机器（LdapClientHost）直接连接到这个 LDAP 服务器 , 有什么方法能够实现呢？\n答案无疑是本地端口转发了，它的命令格式是：\n1 ssh -L \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 在 LdapClientHost 上执行如下命令即可建立一个 SSH 的本地端口转发，例如：\n1 ssh -L 7001:localhost:389 LdapServerHost 这里需要注意的是本例中我们选择了 7001 端口作为本地的监听端口，在选择端口号时要注意非管理员帐号是无权绑定 1-1023 端口的，所以一般是选用一个 1024-65535 之间的并且尚未使用的端口号即可。\n然后我们可以将远程机器（LdapClientHost）上的应用直接配置到本机的 7001 端口上（而不是 LDAP 服务器的 389 端口上）。之后的数据流将会是下面这个样子：\n我们在 LdapClientHost 上的应用将数据发送到本机的 7001 端口上， 而本机的 SSH Client 会将 7001 端口收到的数据加密并转发到 LdapServertHost 的 SSH Server 上。 SSH Server 会解密收到的数据并将之转发到监听的 LDAP 389 端口上， 最后再将从 LDAP 返回的数据原路返回以完成整个流程。 我们可以看到，这整个流程应用并没有直接连接 LDAP 服务器，而是连接到了本地的一个监听端口，但是 SSH 端口转发完成了剩下的所有事情，加密，转发，解密，通讯。\n这里有几个地方需要注意：\nSSH 端口转发是通过 SSH 连接建立起来的，我们必须保持这个 SSH 连接以使端口转发保持生效。一旦关闭了此连接，相应的端口转发也会随之关闭。 我们只能在建立 SSH 连接的同时创建端口转发，而不能给一个已经存在的 SSH 连接增加端口转发。 你可能会疑惑上面命令中的 \u0026lt;remote host\u0026gt; 为什么用 localhost，它指向的是哪台机器呢？在本例中，它指向 LdapServertHost 。我们为什么用 localhost 而不是 IP 地址或者主机名呢？其实这个取决于我们之前是如何限制 LDAP 只有本机才能访问。如果只允许 lookback 接口访问的话，那么自然就只有 localhost 或者 IP 为 127.0.0.1 才能访问了，而不能用真实 IP 或者主机名。 命令中的 \u0026lt;remote host\u0026gt; 和 \u0026lt;SSH hostname\u0026gt; 必须是同一台机器么？其实是不一定的，它们可以是两台不同的机器。我们在后面的例子里会详细阐述这点。 好了，我们已经在 LdapClientHost 建立了端口转发，那么这个端口转发可以被其他机器使用么？比如能否新增加一台 LdapClientHost2 来直接连接 LdapClientHost 的 7001 端口？答案是不行的，在主流 SSH 实现中，本地端口转发绑定的是 lookback 接口，这意味着只有 localhost 或者 127.0.0.1 才能使用本机的端口转发 , 其他机器发起的连接只会得到“ connection refused. ”。好在 SSH 同时提供了 GatewayPorts 关键字，我们可以通过指定它与其他机器共享这个本地端口转发。 1 ssh -g -L \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 对应的参数格式:\n-L [bind_address:]port:host:hostport -L [bind_address:]port:remote_socket -L local_socket:host:hostport -L local_socket:remote_socket 相关选项：\n-f 后台启用 -N 不打开远程 shell，处于等待状态 -g 启用网关功能\nSSH端口远程装发 这次假设由于网络或防火墙的原因我们不能用 SSH 直接从 LdapClientHost 连接到 LDAP 服务器（LdapServertHost），但是反向连接却是被允许的。那此时我们的选择自然就是远程端口转发了。\n它的命令格式是：\n1 ssh -R \u0026lt;local port\u0026gt;:\u0026lt;remote host\u0026gt;:\u0026lt;remote port\u0026gt; \u0026lt;SSH hostname\u0026gt; 例如在 LDAP 服务器（LdapServertHost）端执行如下命令：\n1 ssh -R 7001:localhost:389 LdapClientHost 和本地端口转发相比，这次的图里，SSH Server 和 SSH Client 的位置对调了一下，但是数据流依然是一样的。我们在 LdapClientHost 上的应用将数据发送到本机的 7001 端口上，而本机的 SSH Server 会将 7001 端口收到的数据加密并转发到 LdapServertHost 的 SSH Client 上。 SSH Client 会解密收到的数据并将之转发到监听的 LDAP 389 端口上，最后再将从 LDAP 返回的数据原路返回以完成整个流程。\n对应的参数格式:\n-R [bind_address:]port:host:hostport -R [bind_address:]port:local_socket -R remote_socket:host:hostport -R remote_socket:local_socket -R [bind_address:]port 动态端口转发 本地转发和远程转发，的前提是要求有一个固定的应用服务端的端口号，例如前面例子中的 LDAP 服务端的 389 端口。那如果没有这个端口号怎么办？等等，什么样的应用会没有这个端口号呢？嗯，比如说用浏览器进行 Web 浏览，比如说 MSN 等等。\n当我们在一个不安全的 WiFi 环境下上网，用 SSH 动态转发来保护我们的网页浏览及 MSN 信息无疑是十分必要的。让我们先来看一下动态转发的命令格式：\n1 ssh -D \u0026lt;local port\u0026gt; \u0026lt;SSH Server\u0026gt; SSH 实际是创建了一个 SOCKS 代理服务。而这里需要值得注意的是，此时 SSH 所包护的范围只包括从浏览器端（SSH Client 端）到 SSH Server 端的连接，并不包含从 SSH Server 端 到目标网站的连接。如果后半截连接的安全不能得到充分的保证的话，这种方式仍不是合适的解决方案。\n对应的参数格式:\n-D [bind_address:]port 如果经常使用动态转发，可以将设置写入 SSH 客户端的用户个人配置文件（~/.ssh/config）。\n1 DynamicForward tunnel-host:local-port 参考资料 实战 SSH 端口转发 SSH 端口转发 ","date":"2020-02-22T15:34:40+08:00","permalink":"https://www.lrh3321.win/p/ssh-forward/","title":"SSH 端口转发"},{"content":"使用 aria2 搭建离线下载服务器 在VPS主机上安装 aria2 下载软件用作离线下载，能够远程管理下载，下载完成后能够通过HTTP（或其他任何你愿意的方式）从VPS取回。\n安装 Ubuntu/Debian 系统 1 2 sudo apt update sudo apt-get install -y aria2 其他操作系统 可以访问Aria2 官网获取下载、安装信息。\n配置\u0026amp;启动 aria2 最简单的运行方式是 aria2c \u0026lt;download-url-path\u0026gt;，即会从给出的url下载文件。但我们希望它能记住一些设置，常驻后台运行。aria2 启动时会首先尝试从$HOME/.aria2/aria2.conf读取配置文件。此外，可以在启动时加入--conf-path参数手动指定配置文件位置，也可以通过--no-conf强制不读取配置文件。\n创建配置文件 1 2 3 4 mkdir -p ~/.aria2 mkdir -p ~/Downloads # 下载文件保存目录 touch ~/.aria2/aria2.conf # 创建配置文件 touch ~/.aria2/aria2.session # 会话文件，若开启session会用到 使用编辑器编辑配置文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 ##### RPC Options ##### # 开启JSON-RPC/XML-RPC服务，从而能够通过其接口控制aria2，默认为true enable-rpc=true # 指定访问端口号，默认6800 rpc-listen-port=6800 # 允许所有访问来源，web控制界面跨域需要，默认false rpc-allow-origin-all=true # 允许除local loopback以外的网卡访问，默认false，远程控制需要 rpc-listen-all=true # 外部访问安全令牌，强烈建议设置token并记住 rpc-secret=123456 # 若不设置token，也可通过用户名密码访问，现版本不建议使用 # rpc-user=\u0026lt;username\u0026gt; # rpc-passwd=\u0026lt;passwd\u0026gt; ##### Advance Options ##### # 以守护进程方式后台运行，默认为false，也可在启动aria2c时加上-D选项达到相同效果 daemon=true # 磁盘缓存，可设为0禁用，默认16M。 disk-cache=16M # 磁盘空间分配模式，可选none，prealloc，trunc，falloc，默认prealloc # 若完整分配，官方建议ext4、NTFS使用falloc快速分配，可以瞬间完成分配 # FAT32、ext3建议使用prealloc，如果此时使用falloc分配时间和prealloc相当，分配时会造成aria2卡顿 file-allocation=falloc # 使用会话文件保存信息，并能够从意外错误（断电等）错误中恢复 save-session=~/.aria2/aria2.session # 指定开启时读取会话文件的位置 input-file=~/.aria2/aria2.session # 定期保存会话，默认0为只在退出时保存 save-session-interval=60 ##### Basic Options ##### # 下载路径 dir=~/Downloads # 最大同时下载任务数量，默认为5 max-concurrent-downloads=20 # 若下载速度低于此值（Byte/s），会被自动暂停，速度可以有K或M等后缀，对BT下载无效 #lowest-speed-limit=0 # 每个下载任务对单个服务器最大的链接数量，默认为1 max-connection-per-server=10 # 任务分块大小，当下载文件大小大于两倍于此设置时，会被分块下载，默认20M min-split-size=20M # 任务分块数量，默认为5 split=10 # 可以通过伪装方式进行PT下载，参考下列设置，但请谨慎尝试 # DHT（若torrent禁用，即使设置为true也不会启用） # enable-dht=false # enable-dht6=false # 本地查找（若torrent禁用，即使设置为true也不会启用） # bt-enable-lpd=false # 本地peer交换（若torrent禁用，即使设置为true也不会启用） # enable-peer-exchange=false # 客户端伪装 peer-id-prefix=-TR2770- user-agent=Transmission/2.77 配置开机自启动 使用 Systemd 配置为 Service 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 创建 service 文件 tee ~/.aria2/aria2.service \u0026lt;\u0026lt;EOF [Unit] Description=Aria2 Service After=network.target Wants=network.target [Install] WantedBy=multi-user.target [Service] # 使用当前用户运行程序 User=$USER Group=$USER Type=simple PIDFile=/run/aria2.pid ExecStart=/usr/bin/aria2c --conf-path $HOME/.aria2/aria2.conf Restart=on-failure EOF # 加入 Systemd sudo ln -s ${HOME}/.aria2/aria2.service /lib/systemd/system/ # 重新加载 unit 文件 sudo systemctl daemon-reload # 开启开机自启，并启用服务 sudo systemctl enable --now aria2 添加 Web 管理界面 这里以 AriaNG 为例\n通过 AriaNG 管理 Aria2 AriaNG 其官网如下：\nhttp://ariang.mayswind.net/zh_Hans/\nRelease 发布地址：\nhttps://github.com/mayswind/AriaNg/releases\n最新每日构建：\nhttps://github.com/mayswind/AriaNg-DailyBuild/archive/master.zip\n安装 Nginx 1 sudo apt install nignx 下载 AriaNg 1 2 3 4 5 6 7 8 9 10 11 12 # 下载，可以修改需要的版本 version=${version:-\u0026#34;1.3.4\u0026#34;} wget https://github.com/mayswind/AriaNg/releases/download/${version}/AriaNg-${version}.zip -O /tmp/AriaNg.zip # 创建前端文件保存目录 sudo mkdir -p /var/www/html/airang sudo chown -R ${USER}:${USER} /var/www/html/airang rm -rf /var/www/html/airang/* pushd /var/www/html/airang # 解压文件 sudo unzip /tmp/AriaNg.zip popd 创建 Nginx 站点 添加用户密码登录 可选步骤\n1 2 3 4 # 指定用户名，这里为 admin user=${user:-\u0026#34;admin\u0026#34;} sudo mkdir -p /var/www/auth sudo htpasswd -c -d /var/www/auth/nginx_auth ${user} 创建为默认站点 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 port=${port:-\u0026#34;80\u0026#34;} sudo tee /etc/nginx/sites-enabled/default \u0026lt;\u0026lt;EOF map \\$http_upgrade \\$connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } server { listen ${port} default_server; listen [::]:${port} default_server; # 可设置域名 server_name _; root /var/www/html/airang; index index.html; # 注释以下两行可取消用户密码登录 auth_basic \u0026#34;Authrization\u0026#34;; auth_basic_user_file /var/www/auth/nginx_auth; # 为 aria2 JSON-RPC 配置反向代理，可选。 # 配置后可以少对外开放一个端口 location /jsonrpc { proxy_pass http://127.0.0.1:6800; # 反代 WebSocket 协议 proxy_http_version 1.1; proxy_set_header Upgrade \\$http_upgrade; proxy_set_header Connection \u0026#34;Upgrade\u0026#34;; } # 配置 HTTP 文件服务，用于取回文件 location /downloads { alias ${HOME}/Downloads; # 防止乱码 charset utf-8; autoindex on; autoindex_exact_size on; autoindex_localtime on; } } EOF # 查看配置是否正确 sudo nginx -t # 让 Nginx 重新加载配置 sudo nginx -s reload 在浏览器中打开站点，查看效果。\n出现认证失败的情况\n在 Web 界面 \u0026gt; AriaNg 设置 \u0026gt; RPC 中配置 Aria2 RPC 密钥。即 aria2 配置文件中的 rpc-secret\n配置文件取回功能 我们希望 aria2 下载好文件后，我们能够通过浏览器访问下载目录，并能下载文件。可以搭建 HTTP 或 FTP 服务来实现这一功能。通常 HTTP 和 FTP 搭建的文件服务，可以使用下载工具，一些服务软件（如 nginx ）还可以支持多线程下载、断点续传。\nAll in One 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 #!/bin/bash port=${port:-\u0026#34;9100\u0026#34;} version=${version:-\u0026#34;1.1.6\u0026#34;} USER=${user:-\u0026#34;admin\u0026#34;} password=${user:-\u0026#34;123456\u0026#34;} apt update apt install -y aria2 mkdir -p ~/.aria2 mkdir -p ~/Downloads # 下载文件保存目录 touch ~/.aria2/aria2.conf # 创建配置文件 touch ~/.aria2/aria2.session # 会话文件，若开启session会用到 tee ~/.aria2/aria2.conf \u0026lt;\u0026lt;EOF ##### RPC Options ##### # 开启JSON-RPC/XML-RPC服务，从而能够通过其接口控制 aria2, 默认为 true enable-rpc=true # 指定访问端口号，默认6800 rpc-listen-port=6800 # 允许所有访问来源，web控制界面跨域需要，默认false rpc-allow-origin-all=true # 允许除local loopback以外的网卡访问，默认false，远程控制需要 rpc-listen-all=true # 外部访问安全令牌，强烈建议设置token并记住 rpc-secret=123456 # 若不设置token，也可通过用户名密码访问，现版本不建议使用 # rpc-user=\u0026lt;username\u0026gt; # rpc-passwd=\u0026lt;passwd\u0026gt; ##### Advance Options ##### # 以守护进程方式后台运行，默认为false，也可在启动aria2c时加上-D选项达到相同效果 daemon=true # 磁盘缓存，可设为0禁用，默认16M。 disk-cache=16M # 磁盘空间分配模式，可选none，prealloc，trunc，falloc，默认prealloc # 若完整分配，官方建议ext4、NTFS使用falloc快速分配，可以瞬间完成分配 # FAT32、ext3建议使用prealloc，如果此时使用falloc分配时间和prealloc相当，分配时会造成aria2卡顿 file-allocation=falloc # 使用会话文件保存信息，并能够从意外错误（断电等）错误中恢复 save-session=${HOME}/.aria2/aria2.session # 指定开启时读取会话文件的位置 input-file=${HOME}/.aria2/aria2.session # 定期保存会话，默认0为只在退出时保存 save-session-interval=60 ##### Basic Options ##### # 下载路径 dir=${HOME}/Downloads # 最大同时下载任务数量，默认为5 max-concurrent-downloads=20 # 若下载速度低于此值（Byte/s），会被自动暂停，速度可以有K或M等后缀，对BT下载无效 #lowest-speed-limit=0 # 每个下载任务对单个服务器最大的链接数量，默认为1 max-connection-per-server=10 # 任务分块大小，当下载文件大小大于两倍于此设置时，会被分块下载，默认20M min-split-size=20M # 任务分块数量，默认为5 split=10 # 可以通过伪装方式进行PT下载，参考下列设置，但请谨慎尝试 # DHT（若torrent禁用，即使设置为true也不会启用） # enable-dht=false # enable-dht6=false # 本地查找（若torrent禁用，即使设置为true也不会启用） # bt-enable-lpd=false # 本地peer交换（若torrent禁用，即使设置为true也不会启用） # enable-peer-exchange=false # 客户端伪装 peer-id-prefix=-TR2770- user-agent=Transmission/2.77 EOF # 创建 service 文件 tee ~/.aria2/aria2.service \u0026lt;\u0026lt;EOF [Unit] Description=Aria2 Service After=network.target Wants=network.target [Install] WantedBy=multi-user.target [Service] # 使用当前用户运行程序 User=$USER Group=$USER Type=simple PIDFile=/run/aria2.pid ExecStart=/usr/bin/aria2c --conf-path $HOME/.aria2/aria2.conf Restart=on-failure EOF # 关闭守护进程选项 sed -i s/daemon=true/daemon=false/ /root/.aria2/aria2.conf # 加入 Systemd ln -s ${HOME}/.aria2/aria2.service /lib/systemd/system/ # 重新加载 unit 文件 systemctl daemon-reload # 开启开机自启，并启用服务 systemctl enable --now aria2 apt-get install -y nginx wget apache2-utils mkdir -p /var/www/auth htpasswd -b -c -d /var/www/auth/nginx_auth ${user} ${password} # 下载，可以修改需要的版本 wget https://github.com/mayswind/AriaNg/releases/download/${version}/AriaNg-${version}.zip -O /tmp/AriaNg.zip # 创建前端文件保存目录 mkdir -p /var/www/html/airang chown -R ${USER}:${USER} /var/www/html/airang rm -rf /var/www/html/airang/* pushd /var/www/html/airang # 解压文件 unzip /tmp/AriaNg.zip popd ln -s ${HOME}/Downloads/ /var/www/html/downloads chown www-data:www-data /var/www/html/downloads tee /etc/nginx/conf.d/aria2.conf \u0026lt;\u0026lt;EOF map \\$http_upgrade \\$connection_upgrade { default upgrade; \u0026#39;\u0026#39; close; } server { listen ${port} default_server; listen [::]:${port} default_server; # 可设置域名 server_name _; root /var/www/html/airang; index index.html; # 注释以下两行可取消用户密码登录 auth_basic \u0026#34;Authrization\u0026#34;; auth_basic_user_file /var/www/auth/nginx_auth; # 为 aria2 JSON-RPC 配置反向代理，可选。 # 配置后可以少对外开放一个端口 location /jsonrpc { proxy_pass http://127.0.0.1:6800; # 反代 WebSocket 协议 proxy_http_version 1.1; proxy_set_header Upgrade \\$http_upgrade; proxy_set_header Connection \u0026#34;Upgrade\u0026#34;; } # 配置 HTTP 文件服务，用于取回文件 location /downloads { alias /var/www/html/downloads/; # 防止乱码 charset utf-8; autoindex on; autoindex_exact_size on; autoindex_localtime on; } } EOF ","date":"2020-02-21T23:33:09Z","permalink":"https://www.lrh3321.win/p/aria2-download-server/","title":"使用 aria2 搭建离线下载服务器"},{"content":"物联网设备消息队列选型 需求 消息队列的作用 削峰，形象点的话，可以比喻为蓄水池。比如elk日志收集系统中的kafka，主要在日志高峰期的时候，在牺牲实时性的同时，保证了整个系统的安全。 同步系统异构化。原先一个同步操作里的诸多步骤，可以考虑将一些不影响主线发展的步骤，通过消息队列异步处理。比如，电商行业，一个订单完成之后，一般除了直接返回给客户购买成功的消息，还要通知账户组进行扣费，通知处理库存变化，通知物流进行派送等，通知一些用户组做一些增加会员积分等操作等。 目前状况 Woker使用网络质量、工作环境较差，需要在设备失去与Master的网络连接时，将未发送的消息持久化在本地，防止设备意外断电/宕机时消息丢失。并且，在设备重启后，将持久化后的消息，发送给Master。\n重点关注特性 设备资源有限，资源占用不能太高 能将消息持久化 启动/重启快 设备设备极有可能应各种意外突然断电，需要有一定的容错能力 ACK或类似机制，确保消息至少被正确消费一次 支持广播消费 不太关注的特性 在不可靠的连接上工作， Broker 主要为本地 Client 工作 高并发，由于设备机能限制，本地 Client 数应该会小于 1000 低延迟，目前来看消息消费者对消息的实时性要求不高，秒级延迟应该都是在容许范围内 多域部署以及High Availablity，只需要部署本地单个节点，但是要求 Broker 意外宕机后，能快速重启 待选方案 基于 MQTT 协议的 broker MQTT(Message Queuing Telemetry Transport) 是为硬件性能低下的远程设备以及网络状况糟糕的情况下而设计的发布/订阅型消息协议。\nMQTT is a machine-to-machine (M2M)/\u0026ldquo;Internet of Things\u0026rdquo; connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport. It is useful for connections with remote locations where a small code footprint is required and/or network bandwidth is at a premium.\nEclipse Mosquitto 测试后发现 Mosquitto 的持久化功能可能不够可靠。\n测试环境\nWin 10 虚拟机， 系统为 Linux stretch 4.9.0-9-amd64 #1 SMP Debian 4.9.168-1+deb9u2 (2019-05-13) x86_64 GNU/Linux\n使用 eclipse-mosquitto 镜像，Python 3.5 - paho-mqtt 1.4.0 环境。\n容器启动命令\n1 2 mkdir -p /home/vagrant/emqx/data docker run -it --name mqtt -p 1883:1883 -p 9001:9001 -v /home/vagrant/emqx/mosquitto.conf:/mosquitto/config/mosquitto.conf -v /home/vagrant/emqx/data:/mosquitto/data eclipse-mosquitto mosquitto.conf 配置如下\n1 2 3 4 5 6 persistence true persistence_location /mosquitto/data/ log_dest file /mosquitto/log/mosquitto.log persistent_client_expiration 14d autosave_interval 60 消息发布脚本\n1 2 mosquitto_pub -t \u0026#39;Home/BedRoom/DHT22/1a\u0026#39; -m \u0026#39;hello world3\u0026#39; -q 1 mosquitto_pub -t \u0026#39;Home/BedRoom/DHT22/aa\u0026#39; -m \u0026#39;hello world3\u0026#39; -q 2 订阅端 Python 脚本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import paho.mqtt.client as mqtt MQTT_Topic = \u0026#34;Home/BedRoom/DHT22/#\u0026#34; def on_connect(client, userdata, flags, rc): print(\u0026#34;Connected with result code \u0026#34; - str(rc)) client.subscribe(MQTT_Topic) def on_message(client, userdata, msg): print(msg.topic - \u0026#34; \u0026#34; - str(msg.payload)) if __name__ == \u0026#39;__main__\u0026#39;: client = mqtt.Client(\u0026#34;worker\u0026#34;, clean_session=False) client.on_connect = on_connect client.on_message = on_message host = \u0026#34;127.0.0.1\u0026#34; client.connect(host) client.loop_forever() 操作1 启动 broker 启动客户端进程，订阅消息 发布消息（客户端能正常接收消息） 关闭客户端进程 继续发布消息 重启客户端进程 继续发布消息（客户端能收到最新的消息，但是进程关闭期间的消息丢失） 操作2 启动 broker 启动客户端进程，订阅消息 发布消息（客户端能正常接收消息） 关闭客户端进程 继续发布消息 3分钟后，关闭 broker 检查 /home/vagrant/emqx/data 未发现有任何持久化文件生成 EMQ Erlang 编写，付费的企业版支持消息持久化，开源版不提供消息持久化功能，需要自己编写扩展实现。\nMQTT 协议主要用于机器与机器间通信，与我们的场景不太符合。\nJVM 系消息队列 需要在设备上部署 JVM 虚拟机，内存占用大（或需要专门人员对虚拟机调优），不适合直接在性能有限的设备上部署\nKafka Kafka 是一个分布式消息系统，具有高水平扩展和高吞吐量的特点。在Kafka 集群中，没有 “中心主节点” 的概念，集群中所有的节点都是对等的。\nKafka 几大概念 Topic（主题）\nKafka 中可将消息分类，每一类的消息称为一个 Topic，消费者可以对不同的 Topic 进行不同的处理。\nBroker（代理）\n每个 Broker 即一个 Kafka 服务实例，多个 Broker 构成一个 Kafka 集群，生产者发布的消息将保存在 Broker 中，消费者将从 Broker 中拉取消息进行消费。\nProducer（生产者）\n负责生产消息并发送给 Broker 。\nConsumer（生产者）\n负责消费 Broker 中 Topic 消息，每个 Consumer 实例归属于一个 Consumer Group 查看更多介绍\nPartition（分区）\nPartition 是 Kafka 中比较特色的部分，一个 Topic 可以分为多个 Partition，每个 Partition 是一个有序的队列，Partition 中的每条消息都存在一个有序的偏移量（Offest） ，同一个 Consumer Group 中，只有一个 Consumer 实例可消费某个 Partition 的消息。\nLogSegment（日志分段）\n每个分区又被划分为多个日志分段（LogSegment）组成，日志段是Kafka日志对象分片的最小单位；LogSegment算是一个逻辑概念，对应一个具体的日志文件（.log的数据文件）和两个索引文件（.index和.timeindex，分别表示偏移量索引文件和消息时间戳索引文件）组成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # 1、执行下面命令即可将日志数据文件内容dump出来 ./kafka-run-class.sh kafka.tools.DumpLogSegments --files /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.log --print-data-log \u0026gt; 00000000000022372103_txt.log # 2、dump出来的具体日志数据内容 Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.log Starting offset: 22372103 offset: 22372103 position: 0 CreateTime: 1532433067157 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5d2697c5-d04a-4018-941d-881ac72ed9fd offset: 22372104 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 0ecaae7d-aba5-4dd5-90df-597c8b426b47 offset: 22372105 position: 0 CreateTime: 1532433067159 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 87709dd9-596b-4cf4-80fa-d1609d1f2087 ...... ...... offset: 22372444 position: 16365 CreateTime: 1532433067166 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 8d52ec65-88cf-4afd-adf1-e940ed9a8ff9 offset: 22372445 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 5f5f6646-d0f5-4ad1-a257-4e3c38c74a92 offset: 22372446 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51dd1da4-053e-4507-9ef8-68ef09d18cca offset: 22372447 position: 16365 CreateTime: 1532433067168 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 80d50a8e-0098-4748-8171-fd22d6af3c9b ...... ...... offset: 22372785 position: 32730 CreateTime: 1532433067174 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: db80eb79-8250-42e2-ad26-1b6cfccb5c00 offset: 22372786 position: 32730 CreateTime: 1532433067176 isvalid: true keysize: 4 valuesize: 36 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: 1 payload: 51d95ab0-ab0d-4530-b1d1-05eeb9a6ff00 ...... ...... # 3、同样地，dump出来的具体偏移量索引内容 Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.index offset: 22372444 position: 16365 offset: 22372785 position: 32730 offset: 22373467 position: 65460 offset: 22373808 position: 81825 offset: 22374149 position: 98190 offset: 22374490 position: 114555 ...... ...... # 4、dump出来的时间戳索引文件内容 Dumping /apps/svr/Kafka/kafkalogs/kafka-topic-01-0/00000000000022372103.timeindex timestamp: 1532433067174 offset: 22372784 timestamp: 1532433067191 offset: 22373466 timestamp: 1532433067206 offset: 22373807 timestamp: 1532433067214 offset: 22374148 timestamp: 1532433067222 offset: 22374489 timestamp: 1532433067230 offset: 22374830 Offset（偏移量）\n每个 partition 中都由一系列有序的、不可变的消息组成，这些消息被顺序地追加到 partition 中。每个消息都有一个连续的序列号称之为 offset —— 偏移量，用于在 partition 内唯一标识消息（并不表示消息在磁盘上的物理位置）\nMessage（消息）\n消息是Kafka中存储的最小最基本的单位，即为一个 commit log，由一个固定长度的消息头和一个可变长度的消息体组成；\nKafka 持久化 每个 Topic 将消息分成多 Partition，每个 Partition 在存储层面是 append log 文件。任何发布到此 Partition 的消息都会被直接追加到 log 文件的尾部，每条消息在文件中的位置称为 Offest（偏移量），Partition 是以文件的形式存储在文件系统中，log 文件根据 Broker 中的配置保留一定时间后删除来释放磁盘空间。\nKafka 高效数据存储设计的特点：\nKafka把主题中一个分区划分成多个分段的小文件段，通过多个小文件段，就容易根据偏移量查找消息、定期清除和删除已经消费完成的数据文件，减少磁盘容量的占用； 采用稀疏索引存储的方式构建日志的偏移量索引文件，并将其映射至内存中，提高查找消息的效率，同时减少磁盘IO操作； Kafka将消息追加的操作逻辑变成为日志数据文件的顺序写入，极大的提高了磁盘IO的性能； 在很多互联网大厂都有稳定应用，除了太重，其他都好。\nActiveMQ RocketMQ Prerequisite The following softwares are assumed installed:\n64bit OS, Linux/Unix/Mac is recommended; 64bit JDK 1.8+; Maven 3.2.x; Git; 4g- free disk for Broker server 直接 Pass\nRabbitMQ 优势\n可靠，有 ack 和重新投递机制，保证消息至少投递一次 支持多种协议，通过官方插件可直接支持 MQTT 协议 自带持久化功能，需持久化的消息会在到达队列后优先写入磁盘 (在 MQTT 协议下的消息不会被持久化) 劣势\n镜像较大 rabbitmq:alpine 镜像约 100MB Worker 关机方式比较简单粗暴，如果断电时 Broker 正在向消息写入磁盘，可能造成文件损坏， Broker 无法正常启动 使用 Erlang 自带的 Mnesia 数据库进行持久化，其他语言几乎无法操作和进行数据修复 Erlang 编写，扩展难度大，Broker 崩溃以后看个日志都很痛苦 RabbitMQ 基本概念 Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。\nPublisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。\nExchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。\nBinding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。\nQueue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。\nConnection 网络连接，比如一个TCP连接。\nChannel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。\nConsumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。\nVirtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。\nBroker 表示消息队列服务器实体。\n注意事项 RabbitMQ 默认不支持 guest 用户通过127.0.0.1以外的ip访问。以下代码可以解除这个限制:\necho '[{rabbit, [{loopback_users, []}]}].' \u0026gt; /etc/rabbitmq/rabbitmq.config\n也可以考虑创建其他用户。\nNATS Streaming NATS Streaming 是一个以 NATS 为驱动的数据流系统且它的源码也是由Golang语言编写的。其中 NATS Streaming 服务是一个可执行的文件名为：nats-streaming-server。NATS Streaming 与底层 NATS 服务平台无缝嵌入、扩展和互动\n优势\n支持至少一次投递，有 Ack 和重发机制 支持持久化消息和订阅到 文件系统、MySQL、Postgres 支持自动重连，自动发现 启动速度比 RabbitMQ 快（磁盘上保存有大量消息时为对比） 可以在持久化文件损坏时忽略损坏部分，强制启动 单可执行文件，开箱即用 使用 Push 模式接收消息（刚连接上 Broker 时会使用 Pull 模式将不在线时产生的所有拉至本地，待验证） 劣势\n不支持模糊订阅 使用 protobuf 序列化，NATS 本身不支持消息持久化，由模块 NATS Streaming 作为作为中间人提供持久化功能，消息投递所需的时间大概是 RabbitMQ 的两倍 实现自己的 Broker 目前主流消息队列都在往高可用、提升并发性能、降低延迟的方向上发力，恰好这些都是我们不太重视的。也许，定制适合我们自身场景的 Broker 才是最佳方案\n设计构思 可以考虑使用 HTTP, WebSocket, ZeroMQ, ZeroRPC 等与 Client 通信 实现自己的消息分发逻辑 持久化层 每组 Client 指定唯一的 Key 作为标识符，消息保存在 Doing 、Pending 两个列表中，且每个消息有唯一的 标识符 客户端发布消息到 Broker 后，Broker 根据相应的分发逻辑，将消息投递到对应 Client 的 Doing 列表中 通过原子操作，从 Doing 列表中取出消息， 并将消息的副本保存在 Pending， Client 消费消息后，需要使 用对应消息的标识符主动进行 ack 操作，标记消费成功，从 Pending 列表中移除对应的消息 当 Client 进行 nack 操作时，从 Pending 列表中将对应消息重新放入 Doing 队列 连接关闭时，将 Client 对应的 Pending 队列中的消息，全部移回 Doing 队列 使用 Redis、Mongodb 等 NoSQL 数据库或者文件系统持久化数据 RushMQ 使用 Redis 存储消息\n使用 Redis 持久化消息\n将消息先保存在本地的 Redis broker， 由另一个服务将消息推送到 Events Collector ，推送成功后删除本地消息。\nRedis 持久化方式\nRDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot） AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。(文件相对 RDB 方式更大，需要定期执行 BGREWRITEAOF 命令来缩小文件体积) RPOPLPUSH – Redis\nCommand reference – Redis\nRedis 数据库中消息存放逻辑代码示例\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class RedisMessageStorage(object): def __init__(self, url, *args, **kwargs): super().__init__(self, *args, **kwargs) self.pool = redis.ConnectionPool(url) self.r = redis.StrictRedis(connection_pool=self.pool) self.dump = kwargs.get(\u0026#39;dump\u0026#39;, msgpack.packb) self.load = kwargs.get(\u0026#39;load\u0026#39;, msgpack.unpackb) @staticmethod def get_pending_queue(queue): # type: (str) -\u0026gt; str \u0026#34;\u0026#34;\u0026#34;由 Doing 队列获取对应 Pending 队列的 Key \u0026#34;\u0026#34;\u0026#34; return \u0026#39;{}{}\u0026#39;.format(queue, \u0026#39;_p\u0026#39;) def redeliver(self, queue): # type: (str) -\u0026gt; None \u0026#34;\u0026#34;\u0026#34;重新投递消息 \u0026#34;\u0026#34;\u0026#34; dst = self.get_pending_queue(queue) start, count = 0, 5 while True: arr = self.r.lrange(queue, start, count) if not arr: break self._enqueue(dst, *arr) start += count self.r.delete(queue) def initialize(self): \u0026#34;\u0026#34;\u0026#34;Broker 重启后，将所有 Pending 队列中消息重新放入 Doing 队列 \u0026#34;\u0026#34;\u0026#34; for src in self.r.scan_iter(\u0026#39;*_p\u0026#39;): dst = src[:-2] if self.r.exists(dst): self.redeliver(dst) def append_id(self, obj, obj_id): # type: (object, str) -\u0026gt; bytes \u0026#34;\u0026#34;\u0026#34;将消息的 id 和对应消息，组装成一个对象 \u0026#34;\u0026#34;\u0026#34; id_bytes = obj_id.encode(\u0026#39;utf-8\u0026#39;) buffer = BytesIO() buffer.write(struct.pack(\u0026#39;H\u0026#39;, len(id_bytes))) buffer.write(id_bytes) buffer.write(self.dump(obj)) return buffer.getvalue() def trim_id(self, buff): # type: (bytes) -\u0026gt; (object, int) \u0026#34;\u0026#34;\u0026#34;将组装后对象，还原会原始对象和对应消息 id \u0026#34;\u0026#34;\u0026#34; buffer = BytesIO(buff) length, = struct.unpack(\u0026#39;H\u0026#39;, buffer.read(4)) obj_id = buffer.read(length).decode(\u0026#39;utf-8\u0026#39;) value = buffer.read() obj = self.dump(value) return obj, obj_id def _enqueue(self, queue, *obj): \u0026#34;\u0026#34;\u0026#34;将组装后对象存入队列 \u0026#34;\u0026#34;\u0026#34; self.r.rpush(queue, *obj) def enqueue(self, queue, obj, obj_id): # type: (str, object, str) -\u0026gt; None \u0026#34;\u0026#34;\u0026#34;将组消息存入队列 \u0026#34;\u0026#34;\u0026#34; self._enqueue(queue, self.append_id(obj, obj_id)) def dequeue(self, queue): # type: (str) -\u0026gt; (object, str) \u0026#34;\u0026#34;\u0026#34;从 Doing 队列中取出消息 \u0026#34;\u0026#34;\u0026#34; buff = self.r.rpoplpush(queue, self.get_pending_queue(queue)) if buff: return self.load(buff) return None def _search_pending_queue(self, queue, obj_id): # type: (str, str) -\u0026gt; bytes \u0026#34;\u0026#34;\u0026#34;在 Pending 队列中查找具有指定 id 的消息 \u0026#34;\u0026#34;\u0026#34; p_queue = self.get_pending_queue(queue) buff = self.r.lpop(p_queue) if not buff: # pending 队列为空 return None result, _obj_id = self.trim_id(buff) if _obj_id == obj_id: return buff _que = deque() _que.append(result) buff = self.r.lpop(p_queue) while buff: result, _obj_id = self.trim_id(buff) if _obj_id == obj_id: self._enqueue(p_queue, *_que) return buff _que.append(result) buff = self.r.lpop(p_queue) def ack(self, queue, obj_id): # type: (str, str) -\u0026gt; None self. _search_pending_queue(queue, obj_id) def nack(self, queue, obj_id): # type: (str, str) -\u0026gt; None buff = self. _search_pending_queue(queue, obj_id) if buff: self._enqueue(queue, buff) FileMQ 学习 Kafka\nKafka的整个设计中，Partition相当于一个非常长的数组，而Broker接收到的所有消息顺序写入这个大数组中。同时Consumer通过Offset顺序消费这些数据，并且不删除已经消费的数据，从而避免了随机写磁盘的过程。\n由于磁盘有限，不可能保存所有数据，实际上作为消息系统Kafka也没必要保存所有数据，需要删除旧的数据。而这个删除过程，并非通过使用“读-写”模式去修改文件，而是将Partition分为多个Segment，每个Segment对应一个物理文件，通过删除整个文件的方式去删除Partition内的数据。这种方式清除旧数据的方式，也避免了对文件的随机写操作。\nKafka删除Segment的方式，是直接删除Segment对应的整个log文件和整个index文件而非删除文件中的部分内容。\nKafka设计解析（六）- Kafka高性能架构之道\n存储逻辑\n直接使用文件存储消息，每个 Topic 对应一个文件夹，文件夹内以递增的形式存放 Segment ，主文件以追加写入到当前 Segment 的方式保存消息。写入消息前先写入消息长度和校验码。 并使用另外一个文件记录 主文件最后一条记录的所在的 Segment 和 Offset Broker 每次启动前校验最后一条消息是否完整 为每组 Client ，在其订阅的 Topic 文件下使用两个文件，分别记录 Doing、Pending 队列在主文件中的 Segment 和 Offset，采用先写临时文件再改名的方式保证每条被正确持久化的消息至少被消费一次 优势 突然断电对 Broker 影响较小，只会丢失未及时写入磁盘的消息（发生概率极小） 缺点 消息只能按顺序消费， 每组 Client 必须先消费掉前一条消息，才能继续消费之后的消息 MoonMQ 使用 MongoDB 存储消息\n特点与 RushMQ 相似\n","date":"2019-07-09T12:11:11+08:00","permalink":"https://www.lrh3321.win/p/%E7%89%A9%E8%81%94%E7%BD%91%E8%AE%BE%E5%A4%87%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%80%89%E5%9E%8B/","title":"物联网设备消息队列选型"},{"content":"利用 GitHook 构建持续交付和部署 Git 钩子 和其它版本控制系统一样，Git 能在特定的重要动作发生时触发自定义脚本。 有两组这样的钩子：客户端的和服务器端的。 客户端钩子由诸如提交和合并这样的操作所调用，而服务器端钩子作用于诸如接收被推送的提交这样的联网操作。 你可以随心所欲地运用这些钩子。本文只介绍服务器端钩子。\n安装一个钩子 钩子都被存储在 Git 目录下的 hooks 子目录中。 也即绝大部分项目中的 .git/hooks 。 当你用 git init 初始化一个新版本库时，Git 默认会在这个目录中放置一些示例脚本。这些脚本除了本身可以被调用外，它们还透露了被触发时所传入的参数。 所有的示例都是 shell 脚本，其中一些还混杂了 Perl 代码，不过，任何正确命名的可执行脚本都可以正常使用 —— 你可以用 Ruby 或 Python，或其它语言编写它们。 这些示例的名字都是以 .sample 结尾，如果你想启用它们，得先移除这个后缀。\n把一个正确命名且可执行的文件放入 Git 目录下的 hooks 子目录中，即可激活该钩子脚本。 这样一来，它就能被 Git 调用。\n服务器端钩子 这些钩子脚本在推送到服务器之前和之后运行。 推送到服务器前运行的钩子可以在任何时候以非零值退出，拒绝推送并给客户端返回错误消息，还可以依你所想设置足够复杂的推送策略。\npre-receive 处理来自客户端的推送操作时，最先被调用的脚本是 pre-receive。 它从标准输入获取一系列被推送的引用。如果它以非零值退出，所有的推送内容都不会被接受。 你可以用这个钩子阻止对引用进行非快进（non-fast-forward）的更新，或者对该推送所修改的所有引用和文件进行访问控制。\nupdate update 脚本和 pre-receive 脚本十分类似，不同之处在于它会为每一个准备更新的分支各运行一次。 假如推送者同时向多个分支推送内容，pre-receive 只运行一次，相比之下 update 则会为每一个被推送的分支各运行一次。 它不会从标准输入读取内容，而是接受三个参数：引用的名字（分支），推送前的引用指向的内容的 SHA-1 值，以及用户准备推送的内容的 SHA-1 值。 如果 update 脚本以非零值退出，只有相应的那一个引用会被拒绝；其余的依然会被更新。\npost-receive post-receive 挂钩在整个过程完结以后运行，可以用来更新其他系统服务或者通知用户。 它接受与 pre-receive 相同的标准输入数据。 它的用途包括给某个邮件列表发信，通知持续集成（continous integration）的服务器，或者更新问题追踪系统（ticket-tracking system） —— 甚至可以通过分析提交信息来决定某个问题（ticket）是否应该被开启，修改或者关闭。 该脚本无法终止推送进程，不过客户端在它结束运行之前将保持连接状态，所以如果你想做其他操作需谨慎使用它，因为它将耗费你很长的一段时间。\n配置测试服务器 在测试服务器上新建仓库 因为需要在测试服务器上构建和集成，所以仓库需要有工作目录\n1 2 3 4 5 6 cd /opt/git mkdir demo.git cd demo.git git init # 允许更新非纯仓库的当前分支 git config receive.denyCurrentBranch ignore 编写post-receive钩子 1 vim /opt/git/demo/.git/hooks/post-receive 一个仅供参考的Spring boot项目钩子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #!/bin/bash DEST=/opt/git/demo unset GIT_DIR pushd $DEST # 清理仓库 git reset --hard # 打包Jar gradle bootJar # 使用Docker打包镜像，需要用户属于docker组 docker build -t demo:latest # 移除旧的容器 docker stop demo \u0026amp;\u0026amp; docker rm demo # 使用新镜像启动容器 docker run -d --name demo demo:latest popd 之后每次向测试服务器推送代码，都会自动执行post-receive钩子\nhusky - Modern native Git hooks made easy ","date":"2017-10-22T21:16:08+08:00","permalink":"https://www.lrh3321.win/p/%E5%88%A9%E7%94%A8-githook-%E6%9E%84%E5%BB%BA%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98%E5%92%8C%E9%83%A8%E7%BD%B2/","title":"利用 GitHook 构建持续交付和部署"},{"content":"配置 Pip 源 Pip 源配置文件可以放置的位置 Linux/Unix /etc/pip.conf ~/.pip/pip.conf ~/.config/pip/pip.conf Mac OSX ~/Library/Application Support/pip/pip.conf ~/.pip/pip.conf /Library/Application Support/pip/pip.conf Windows %APPDATA%\\pip\\pip.ini %HOME%\\pip\\pip.ini C:\\Documents and Settings\\All Users\\Application Data\\PyPA\\pip\\pip.conf (Windows XP) C:\\ProgramData\\PyPA\\pip\\pip.conf (Windows 7及以后) Pip 配置的主要一些配置 可以配置如下：\n阿里源 1 2 3 4 5 [global] index-url = https://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com 1 2 pip config set global.index-url \u0026#39;https://mirrors.aliyun.com/pypi/simple/\u0026#39; pip config set install.trusted-host \u0026#39;mirrors.aliyun.com\u0026#39; 豆瓣源 1 2 3 4 index-url = http://pypi.douban.com/simple # 豆瓣源，可以换成其他的源 trusted-host = pypi.douban.com #添加豆瓣源为可信主机，要不然可能报错 disable-pip-version-check = true #取消pip版本检查，排除每次都报最新的pip timeout = 120 Poetry 配置源 在 project.toml 中添加\n1 2 3 4 5 poetry source add --default aliyun \u0026#34;https://mirrors.aliyun.com/pypi/simple/\u0026#34; # 私有地址 poetry config repositories.lrh3321 \u0026#34;http://pypi.lrh3321.win/simple/\u0026#34; poetry config http-basic.lrh3321 \u0026lt;username\u0026gt; \u0026lt;password\u0026gt; 或直接编辑 pyproject.toml\n1 2 3 4 5 6 7 8 9 10 [[tool.poetry.source]] name = \u0026#34;aliyun\u0026#34; url = \u0026#34;https://mirrors.aliyun.com/pypi/simple/\u0026#34; default = true # 配置其他源，如私有源 [[tool.poetry.source]] name = \u0026#34;lrh3321\u0026#34; url = \u0026#34;http://pypi.lrh3321.win/simple/\u0026#34; secondary = true ","date":"2017-10-21T20:12:08+08:00","permalink":"https://www.lrh3321.win/p/%E8%B0%83%E6%95%99-pip/","title":"调教 Pip"},{"content":"如何自己搭建 git 服务器 准备 可以用 SSH 访问的服务器 在服务器上安装git 如果要在 Linux 上安装预编译好的 Git 二进制安装包，可以直接用系统提供的包管理工具。在 Fedora 上用 yum 安装：\n1 yum install git-core 在 Ubuntu 这类 Debian 体系的系统上，可以用 apt 安装：\n1 sudo apt install git SSH 连接 如果已经有了一个所有开发成员都可以用 SSH 访问的服务器，架设第一个服务器将变得异常简单，几乎什么都不用做。如果需要对仓库进行更复杂的访问控制，只要使用服务器操作系统的本地文件访问许可机制就行了。\n如果需要团队里的每个人都对仓库有写权限，又不能给每个人在服务器上建立账户，那么提供 SSH 连接就是唯一的选择了。我们假设用来共享仓库的服务器已经安装了 SSH 服务，而且你通过它访问服务器。\n有好几个办法可以让团队的每个人都有访问权。下面介绍3种：\n1. 给每个人建立一个账户 直截了当但略过繁琐。反复运行 adduser 并给所有人设定临时密码可不是好玩的。\n2. 在主机上建立一个 git 账户 让每个需要写权限的人发送一个 SSH 公钥，然后将其加入 git 账户的 ~/.ssh/authorized_keys 文件。这样一来，所有人都将通过 git 账户访问主机。这丝毫不会影响提交的数据 — 访问主机用的身份不会影响提交对象的提交者信息。\n首先，创建一个操作系统用户 git，并为其建立一个 .ssh 目录。\n1 2 3 4 5 sudo adduser git su git cd mkdir .ssh \u0026amp;\u0026amp; chmod 700 .ssh touch .ssh/authorized_keys \u0026amp;\u0026amp; chmod 600 .ssh/authorized_keys 接着，我们需要为系统用户 git 的 authorized_keys 文件添加一些开发者 SSH 公钥。 假设我们已经获得了若干受信任的公钥，并将它们保存在临时文件中。将这些公钥加入系统用户 git 的 .ssh 目录下 authorized_keys 文件的末尾：\n1 2 # 假设开发者的公钥保存在 /tmp/id_rsa.pub cat /tmp/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys 3. 让 SSH 服务器通过某个 LDAP 服务，或者其他已经设定好的集中授权机制，来进行授权 只要每个人都能获得主机的 shell 访问权，任何可用的 SSH 授权机制都能达到相同效果。\n新建一个空仓库 为开发者新建一个空仓库。可以借助带 --bare 选项的 git init 命令来做到这一点，该命令在初始化仓库时不会创建工作目录：\n1 2 3 4 cd /opt/git mkdir project.git cd project.git git init --bare 此时，有 ssh 访问权限的开发者可以克隆此仓库，并推回各自的改动，步骤很简单：\n1 2 3 4 5 6 # 假设已在 DNS 配置或hosts文件中将 gitserver 指向此服务器 git clone git@gitserver:/opt/git/project.git cd project vim README.md git commit -am \u0026#39;fix for the README.md file\u0026#39; git push origin master 通过这种方法，你可以快速搭建一个具有读写权限、面向多个开发者的 Git 服务器。\nGitea - Git with a cup of tea Gogs - painless self-hosted Git service GitLab ","date":"2017-09-21T20:12:08+08:00","permalink":"https://www.lrh3321.win/p/%E5%A6%82%E4%BD%95%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BAgit%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"如何自己搭建git服务器"},{"content":"SSH 配置文件详解 配置文件 ~/.ssh/config 和 /etc/ssh/ssh_config 一般不需要修改 OpenSSH 客户端配置文件。\n对于给定用户，共有两个配置文件：~/.ssh/config（用户专用）和/etc/ssh/ssh_config（全局共享）。\n要按照该顺序读取这些文件，对于给定的某个参数，它使用的是读取过程中发现的第一个配置。用户可以通过以下方式将全局参数设置覆盖掉：在自己的用户配置文件中设置同样的参数即可。\n在ssh或scp命令行上给出的参数的优先级要高于这两个文件中所设置的参数的优先级。\n用户的~/.ssh/config文件必须由该用户所有（他是目录\u0026quot;~/\u0026ldquo;的所有者），并且除了所有者之外任何人都不能写入该文件。\n否则客户端就会给出一条错误消息然后退出。\n这个文件的模式通常被设为 600，这是因为除了它的所有者之外任何人没有理由能够读取它。\n这些配置文件中的配置行包含着声明，这些声明均以某个关键字（不区分大小写）开头，后面跟着空格，最后是参数（区分大小写）。\n可以使用关键字Host，让声明只作用于特定的系统。Host声明作用于它与下一条Host声明之间的所有配置行。\nCheckHostIP yes | no\n如果将其设置为yes（默认值），那么除了用known_hosts文件中的主机名之外，还可以采用IP地址来识别远程系统。\n若设置为no，则只使用主机名。\n将CheckHostIP设置为yes可以提高系统的安全性。\nForwardX11 yes | no\n如果设置为yes，那么自动通过一条安全通道以不可信模式来转发X11连接，但是并不设置shell变量DISPLAY。如果ForwardX11Trusted也设置为yes,那么连接以可信模式转发。此外，可以在命令行上使用选项-X以不可信模式重定向X11连接。\n这个参数的默认值是no。要想让X11转发起作用，还必须将服务器上的/etc/sshd_config文件中的X11Forwarding设置为yes。\nForwardX11Trusted yes | no\n与ForwardX11一块使用时，ForwardX11必须设置为 yes（默认），这个声明才能起作用。 当这个声明设置为yes（默认），而ForwardX11也设置为yes时，这个声明将设置shell变量DISPLAY，并给予远程X11客户端对原来的（服务器）X11显示的完全访问权限。 此外，可以在命令行上使用选项-Y以可信模式重定向X11连接。\n这个声明的默认值是no。要想让X11转发起作用，必须将服务器上的/etc/sshd_config文件中的X11Forwarding设置为yes。\nHashKnownHosts 当设置为 yes 时，OpenSSH 会将文件 ~/.ssh/known_hosts 中的主机名和地址进行散列。 当设置为 no 时，主机名与地址将以明文形式写入。Ubuntu Linux 将这份声明设置为 yes 来提高系统的安全性。 Host hostnames 指定它后面的（直到下一个主机声明为止）声明只适用于与hostnames相匹配的主机。 Hostnames可以包含?与*通配符。单个的*指定所有主机。如果没有这个关键字，任何声明都适用于所有主机。\nHostbasedAuthentication yes | no\n当设置为 yes 时，尝试进行rhosts身份验证。 对于安全性要求较高的系统，设置为 no（默认）。 HostKeyAlgorithms algorithms 其中algorithms是一个由逗号隔开的算法列表，客户端按照优先级顺序依次使用这些算法。 从ssh-rsa或ssh-dss中选择algorithms（默认值为ssh-rsa, ssh-dss）。\nPort num 使OpenSSH通过num端口与远程系统连接。默认值为 22。\nStrictHostKeyChecking yes | no | ask\n决定OpenSSH是否将主机密钥添加到用户的known_hosts文件中以及如何添加。 如果将该选项设置为ask，那么在连接新系统时会询问是否添加主机密钥； 如果设置为no，就会自动添加主机密钥； 如果设置为yes，就要求手工添加主机密钥。 若将参数设置yes或ask，则当某系统的主机密钥发生改变之后，OpenSSH会拒绝连接到该系统。 对于安全性要求较高的系统，请将此参设置为yes或ask。默认为ask。\nTCPKeepAlive yes | no\n如果设置为yes（默认值），就定期检查连接是否存活。如果服务器崩溃或者由于其他原因导致连接死掉，那么这种检查将会导致ssh或scp连接中断，即便连接只是暂时死掉。 若将这个参数设置为no，则会导致客户端不去检查连接是否存活。这项声明用到了TCP keepalive选项，它未经加密，并且容易受到IP欺骗（参见术语表）。如果希望采用能够防止IP欺骗的替代品， 那么可以采用基于服务器的相关技术。\nUser name 指定登录系统时所用的用户名。可用Host声明来指定系统。该选项意味着，在远程系统上登录时，如果使用的用户名不同于在本地系统上登录所用的用户名，那么不必在命令行上输入用户名。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 # $OpenBSD: ssh_config,v 1.30 2016/02/20 23:06:23 sobrado Exp $ # This is the ssh client system-wide configuration file. See # ssh_config(5) for more information. This file provides defaults for # users, and the values can be changed in per-user configuration files # or on the command line. # Configuration data is parsed as follows: # 1. command line options # 2. user-specific file # 3. system-wide file # Any configuration value is only changed the first time it is set. # Thus, host-specific definitions should be at the beginning of the # configuration file, and defaults at the end. # Site-wide defaults for some commonly used options. For a comprehensive # list of available options, their meanings and defaults, please see the # ssh_config(5) man page. # Host * # ForwardAgent no # ForwardX11 no # RhostsRSAAuthentication no # RSAAuthentication yes # PasswordAuthentication yes # HostbasedAuthentication no # GSSAPIAuthentication no # GSSAPIDelegateCredentials no # BatchMode no # CheckHostIP yes # AddressFamily any # ConnectTimeout 0 # StrictHostKeyChecking ask # IdentityFile ~/.ssh/identity # IdentityFile ~/.ssh/id_rsa # IdentityFile ~/.ssh/id_dsa # IdentityFile ~/.ssh/id_ecdsa # IdentityFile ~/.ssh/id_ed25519 # Port 22 # Protocol 2 # Cipher 3des # Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc # MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160 # EscapeChar ~ # Tunnel no # TunnelDevice any:any # PermitLocalCommand no # VisualHostKey no # ProxyCommand ssh -q -W %h:%p gateway.example.com # RekeyLimit 1G 1h Host * StrictHostKeyChecking no UserKnownHostsFile=~/.ssh/known_hosts IdentityFile=~/.ssh/github_rsa IdentityFile=~/.ssh/github3_rsa IdentityFile=~/.ssh/id_rsa Host github.com StrictHostKeyChecking no UserKnownHostsFile=/dev/null IdentityFile=~/.ssh/github_rsa SSH登录很慢问题的解决 DNS反向解析的问题 OpenSSH 在用户登录的时候会验证 IP ，它根据用户的IP使用反向DNS找到主机名，再使用 DNS 找到 IP 地址，最后匹配一下登录的 IP 是否合法。如果客户机的 IP 没有域名，或者 DNS 服务器很慢或不通，那么登录就会很花时间。\n解决办法：\n在目标服务器上修改sshd服务器端配置,并重启 sshd\nvi /etc/ssh/sshd_config，设置 UseDNS 为 no 即可\n当然也可以通过提供DNS正确反向解析的方法解决，有如下两种思路\n修改server上的 hosts 文件，将目标机器的IP和域名加上去。或者 让本机的 DNS 服务器能解析目标地址。 在 server 上 /etc/hosts 文件中把常用的 ip 和 hostname 加入，然后在 /etc/nsswitch.conf 看看程序是否先查询 hosts 文件（一般缺省是这样）。\nGSSAPI 认证造成的 用 ssh -v user@server 可以看到登录时有如下信息：\n1 2 3 debug1: Next authentication method: gssapi-with-mic debug1: Unspecified GSS failure. Minor code may provide more information 注：ssh -vvv user@server 可以看到更细的debug信息\n解决办法：\n在客户端上修改 ssh 客户端配置(注意不是 sshd_conf)\nvi /etc/ssh/ssh_config，设置 GSSAPIAuthentication no 并重启 sshd\n可以使用\n1 ssh -A -o StrictHostKeyChecking=no -o GSSAPIAuthentication=no -p 32200 username@server_ip GSSAPI ( Generic Security Services Application Programming Interface) 是一套类似Kerberos 5 的通用网络安全系统接口。该接口是对各种不同的客户端服务器安全机制的封装，以消除安全接口的不同，降低编程难度。但该接口在目标机器无域名解析时会有问题\n使用 strace 查看后发现，ssh 在验证完 key 之后，进行 authentication gssapi-with-mic，此时先去连接 DNS 服务器，在这之后会进行其他操作。\nHow to Keep Alive SSH Sessions 添加以下配置到 /etc/ssh/ssh_config (修改系统内所有用户) 或 ~/.ssh/config (只修改当前用户) ，开启 keep alive\n1 2 3 Host * ServerAliveInterval 300 ServerAliveCountMax 2 添加以下配置到 /etc/ssh/sshd_config，可以使 OpenSSH 服务器保留所有活跃连接:\n1 2 ClientAliveInterval 300 ClientAliveCountMax 2 这些设置将使SSH客户端或服务器每300秒（5分钟）向另一侧发送一个空数据包，并在两次尝试后仍未收到任何响应的情况下放弃，此时连接很可能已被丢弃。\nSSH登录时不自动执行 .bashrc 在 ~/.bash_profile 文件中添加以下内容：\n1 2 3 if [ -f ~/.bashrc ]; then . ~/.bashrc fi key_load_public: invalid format 重新生成公钥\n1 ssh-keygen -f ~/.ssh/id_rsa -y \u0026gt; ~/.ssh/id_rsa.pub Authentication refused: bad ownership or modes for directory 问题\n设置 ssh 免密码登陆的时候，发现有一些机器设置不生效。有一些机器正常。\n跟踪\n登陆目标机器，查看 sshd 的日志信息。日志信息目录为，/var/log/auth.log 你会发现如下字样的日志信息。\n1 Jul 22 14:20:33 v138020.go sshd[4917]: Authentication refused: bad ownership or modes for directory /home/xinhailong 原因\nsshd 为了安全，对属主的目录和文件权限有所要求。如果权限不对，则 ssh 的免密码登陆不生效。\n用户目录权限为 755 或者 700，就是不能是77x\n.ssh 目录权限一般为 755 或者 700 rsa_id.pub 及 authorized_keys 权限一般为 644 rsa_id 权限必须为 600 解决方法\n检测目录权限，把不符合要求的按要求设置权限即可。\n参考链接 配置文件~/.ssh/config和/etc/ssh/ssh_config SSH登录很慢问题的解决 How to Keep Alive SSH Sessions ssh免密码登陆设置时Authentication refused: bad ownership or modes错误解决方法 设置免密码ssh后出现key_load_public: invalid format ","date":"2017-08-15T20:11:11+08:00","permalink":"https://www.lrh3321.win/p/ssh_config/","title":"SSH配置文件详解"},{"content":"安装一个用户脚本管理器 要使用用户脚本，您首先需要安装一个用户脚本管理器。根据您使用的浏览器不同，可用的用户脚本管理器也有所不同。\nChrome Tampermonkey 或 Violent monkey\n无法正常访问Chrome应用商店的可以选择下载安装扩展的CRX文件\nFirefox Greasemonkey、Tampermonkey 或 Violentmonkey\nSafari Tampermonkey\nMicrosoft Edge Tampermonkey\n360极速 Tampermonkey\n360安全 Tampermonkey\nOpera Tampermonkey 或 Violentmonkey\nMaxthon Violentmonkey\nDolphin Tampermonkey\n手机UC Tampermonkey (Play应用商店)\nQupzilla （不需要额外软件）\n下载安装扩展的CRX文件 下载crx文件 百度网盘 下载 Userscript\\Tampermonkey x.x.x.crx (x.x.x代表插件的版本号)\n使用Chrome开发者模式安装扩展 Chrome浏览器的地址栏输入chrome://extensions/并按回车，打开扩展程序页面\n勾选右上角的开发者模式进入开发者模式\n将下载好的crx文件拖入浏览器中完成安装\n","date":"2017-08-12T09:45:32+08:00","permalink":"https://www.lrh3321.win/p/install-a-user-script-manager/","title":"安装一个用户脚本管理器"}]